{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blgayatri/DS_Projects/blob/main/Zomato_ML_Submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Zomato Restaurant Clustering and Sentiment Analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised Machine Learning\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member -**   Lakshmi gayatri Balivada"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This project addresses key challenges in understanding India's dynamic restaurant industry by leveraging Zomato data. The core objective is to extract actionable insights for both consumers and the company.\n",
        "\n",
        "Specifically, this project aims to:\n",
        "\n",
        "1. Segment Zomato restaurants into distinct clusters based on attributes like cuisine, costing, and location, to uncover inherent market structures and restaurant archetypes.\n",
        "\n",
        "2. Analyze customer sentiment from user reviews to gauge overall public perception, pinpointing areas of satisfaction and identifying potential pain points.\n",
        "\n",
        "3. Identify influential critics within the industry by examining reviewer metadata and their associated sentiment patterns, providing insights into key opinion leaders.\n",
        "\n",
        "4. Derive useful conclusions through visualizations that will:\n",
        "\n",
        "  * Empower customers to efficiently discover restaurants best suited to their preferences within their locality.\n",
        "\n",
        "  * Assist Zomato in identifying strategic growth areas, optimizing service offerings, and understanding regional market nuances.\n",
        "\n",
        "Ultimately, this project seeks to transform raw restaurant and review data into strategic intelligence for navigating and improving the Indian food service landscape.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install/Upgrade scikit-learn\n",
        "!pip install scikit-learn --upgrade"
      ],
      "metadata": {
        "id": "eOrwF-3A2BYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pyLDAvis\n",
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "id": "i4tIMXDT3MmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install contractions\n",
        "!pip install contractions"
      ],
      "metadata": {
        "id": "c_Jewq3_3ad-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gensim\n",
        "!pip install gensim"
      ],
      "metadata": {
        "id": "mUlvrrqC3gcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install shap\n",
        "!pip install shap"
      ],
      "metadata": {
        "id": "8CPB90Jv3mH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.cm as cm\n",
        "import seaborn as sns\n",
        "import math\n",
        "import time\n",
        "from wordcloud import WordCloud\n",
        "from scipy.stats import norm\n",
        "from scipy import stats\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.metrics import roc_auc_score, PrecisionRecallDisplay # CORRECTED LINE\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "#importing kmeans\n",
        "from sklearn.cluster import KMeans\n",
        "#importing random forest and XgB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "#Non-negative matrix Factorization\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "#principal component analysis\n",
        "from sklearn.decomposition import PCA\n",
        "#silhouette score\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "#importing stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "#for tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "# for POS tagging(Part of speech in NLP sentiment analysis)\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "#import stemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "#import tfidf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#LDA\n",
        "# import pyLDAvis.sklearn # COMMENTED OUT\n",
        "# from sklearn.decomposition import LatentDirichletAllocation # COMMENTED OUT\n",
        "#importing contraction\n",
        "import contractions\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "#importing shap for model explainability\n",
        "# import shap # COMMENTED OUT\n",
        "#download small spacy model\n",
        "# !python -m spacy download en_core_web_sm # Uncomment if you plan to use spacy\n",
        "# import spacy\n",
        "# The following lines adjust the granularity of reporting.\n",
        "pd.options.display.float_format = \"{:.2f}\".format\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "JsyfhGbd2U3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load Dataset\n",
        "hotel_df = pd.read_csv('/content/drive/MyDrive/Data Science/Projects/Zomato/Zomato Restaurant names and Metadata_mine.csv')\n",
        "review_df = pd.read_csv('/content/drive/MyDrive/Data Science/Projects/Zomato/Zomato Restaurant reviews_mine.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "hotel_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look (Review Data)\n",
        "review_df.head()"
      ],
      "metadata": {
        "id": "pci_w0qbGeah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f'Total observation and feature for restaurant: {hotel_df.shape}')\n",
        "print(f'Total observation and feature for review: {review_df.shape}')"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "print('--- Restaurant Info ---')\n",
        "hotel_df.info()\n",
        "print('\\n' + '='*120 + '\\n')\n",
        "# Dataset Info for review data\n",
        "print('--- Review Info ---')\n",
        "review_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicates, Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print('For Restaurant')\n",
        "print('\\n')\n",
        "print(f\"Data is duplicated ? {hotel_df.duplicated().value_counts()},unique values with {len(hotel_df[hotel_df.duplicated()])} duplication\")\n",
        "print('\\n')\n",
        "print('='*120)\n",
        "print('\\n')\n",
        "print('For Reviews')\n",
        "print('\\n')\n",
        "print(f\"Data is duplicated ? {review_df.duplicated().value_counts()},unique values with {len(review_df[review_df.duplicated()])} duplication\")\n"
      ],
      "metadata": {
        "id": "oWimM-niH-qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting duplicate values\n",
        "print(f' Duplicate data count = {review_df[review_df.duplicated()].shape[0]}')\n",
        "review_df[review_df.duplicated()]"
      ],
      "metadata": {
        "id": "IzUIOv9KIED9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking values for Anerican Wild Things\n",
        "review_df[(review_df['Restaurant'] == 'American Wild Wings')].shape"
      ],
      "metadata": {
        "id": "KfYZBUMRINU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking values for Arena Eleven\n",
        "review_df[(review_df['Restaurant'] == 'Arena Eleven')].shape"
      ],
      "metadata": {
        "id": "oKBY34Y_IRSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print('--- Missing Values in Restaurant Data ---')\n",
        "print(hotel_df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count (Review Data)\n",
        "print('\\n--- Missing Values in Review Data ---')\n",
        "print(review_df.isnull().sum())"
      ],
      "metadata": {
        "id": "q_VK7XZlHzCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "print('\\nVisualizing Missing Values in Restaurant Data:')\n",
        "sns.heatmap(hotel_df.isnull(), cbar=False);\n",
        "plt.title('Missing Values in hotel_df')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing missing values for review data with a Heatmap\n",
        "print('\\nVisualizing Missing Values in Review Data:')\n",
        "sns.heatmap(review_df.isnull(), cbar=False);\n",
        "plt.title('Missing Values in review_df')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h8RLknbSH67r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "Based on the initial hotel_df.info(), review_df.info(), .isnull().sum(), and .duplicated() checks, here's a summary of insights into the Zomato datasets:\n",
        "\n",
        "Restaurant DataSet (hotel_df)\n",
        "* There are 105 total observations (rows) with 6 features (columns).\n",
        "\n",
        "* Missing Values:\n",
        "\n",
        "  * The Collections feature has a significant number of 54 null values, which is over half of the dataset. This will require careful handling (e.g., imputation or dropping).\n",
        "\n",
        "  * The Timings feature has 1 null value.\n",
        "\n",
        "* No Duplicate Values: This dataset contains no duplicate rows, meaning each of the 105 entries represents a unique restaurant.\n",
        "\n",
        "* Data Type Issues:\n",
        "\n",
        "  * The Cost feature is currently an object data type because its values include commas (e.g., '1,300'). It needs to be converted to a numerical type (e.g., float or int) for any mathematical operations or analysis.\n",
        "\n",
        "  * The Timings feature is also an object data type, as it contains text-based operational hours.\n",
        "\n",
        "Review DataSet (review_df)\n",
        "* There are 10,000 total observations (rows) and 7 features (columns).\n",
        "\n",
        "* Missing Values:\n",
        "\n",
        "  * Most features have missing values: Reviewer, Review, Rating, Metadata, and Time.\n",
        "\n",
        "  * Only Pictures and Restaurant features are completely free of nulls.\n",
        "\n",
        "* Duplicate Values:\n",
        "\n",
        "  * There are a total of 36 duplicate rows in this dataset.\n",
        "\n",
        "  * These duplicates are specifically associated with two restaurants: 'American Wild Wings' and 'Arena Eleven'.\n",
        "\n",
        "  * A significant portion of these duplicate rows consist predominantly of null values, suggesting potential data entry errors or incomplete records rather than legitimate identical reviews.\n",
        "\n",
        "* Data Type Issues:\n",
        "\n",
        "  * The Rating feature is currently an object data type (e.g., 'Like', '4.5') but represents ordinal (or numerical) data. It should be converted to a float (numerical) data type to allow for quantitative analysis of ratings.\n",
        "\n",
        "  * The Time feature, indicating when a review was posted, is an object data type and needs to be converted to a datetime object for any time-based analysis, filtering, or sorting."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(f'Features : {hotel_df.columns.to_list()}')"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns review\n",
        "print(f'Features : {review_df.columns.to_list()}')"
      ],
      "metadata": {
        "id": "ibbCcjnBKJHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "hotel_df.describe().T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe review\n",
        "review_df.describe(include='all').T"
      ],
      "metadata": {
        "id": "zyYTxC0LKTrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer:*\n",
        "\n",
        "Attributes:\n",
        "\n",
        "* Zomato Restaurant (hotel_df)\n",
        "\n",
        "* Name: Name of Restaurants\n",
        "\n",
        "* Links: URL Links of Restaurants\n",
        "\n",
        "* Cost: Per person estimated Cost of dining\n",
        "\n",
        "* Collection: Tagging of Restaurants w.r.t. Zomato categories\n",
        "\n",
        "* Cuisines: Cuisines served by Restaurants\n",
        "\n",
        "* Timings: Restaurant Timings\n",
        "\n",
        "Zomato Restaurant Reviews (review_df)\n",
        "\n",
        "* Restaurant: Name of the Restaurant\n",
        "\n",
        "* Reviewer: Name of the Reviewer\n",
        "\n",
        "* Review: Review Text\n",
        "\n",
        "* Rating: Rating Provided by Reviewer\n",
        "\n",
        "* MetaData: Reviewer Metadata - No. of Reviews and followers\n",
        "\n",
        "* Time: Date and Time of Review\n",
        "\n",
        "* Pictures: No. of pictures posted with review"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in hotel_df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",hotel_df[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable for reviews\n",
        "for i in review_df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",review_df[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "P8kYs259IxZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating copy of both the data\n",
        "hotel = hotel_df.copy()\n",
        "review = review_df.copy()"
      ],
      "metadata": {
        "id": "-LVLDFWuI6yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restaurant"
      ],
      "metadata": {
        "id": "wax8-O54I_ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#before changing data type for cost checking values\n",
        "hotel['Cost'].unique()"
      ],
      "metadata": {
        "id": "fr4DNl3_JA_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# changing the data type of the cost function\n",
        "hotel['Cost'] = hotel['Cost'].str.replace(\",\",\"\").astype('int64')"
      ],
      "metadata": {
        "id": "VPSS9ZHXJFIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 costlier restaurant\n",
        "hotel.sort_values('Cost', ascending = False)[['Name','Cost']][:5]"
      ],
      "metadata": {
        "id": "IA-Oi044JI7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 economy restaurant\n",
        "hotel.sort_values('Cost', ascending = False)[['Name','Cost']][-5:]"
      ],
      "metadata": {
        "id": "HvOCX94AJNgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hotels that share same price\n",
        "hotel_dict = {}\n",
        "amount = hotel.Cost.values.tolist()\n",
        "\n",
        "#adding hotel name based on the price by converting it into list\n",
        "for price in amount:\n",
        "    # Get all the rows that have the current price\n",
        "    rows = hotel[hotel['Cost'] == price]\n",
        "    hotel_dict[price] = rows[\"Name\"].tolist()\n",
        "\n",
        "#converting it into dataframe\n",
        "same_price_hotel_df=pd.DataFrame.from_dict([hotel_dict]).transpose().reset_index().rename(\n",
        "    columns={'index':'Cost',0:'Name of Restaurants'})\n",
        "\n",
        "#alternate methode to do the same\n",
        "#same_price_hotel_df = hotel.groupby('Cost')['Name'].apply(lambda x: x.tolist()).reset_index()\n",
        "\n",
        "#getting hotel count\n",
        "hotel_count = hotel.groupby('Cost')['Name'].count().reset_index().sort_values(\n",
        "    'Cost', ascending = False)\n",
        "\n",
        "#merging together\n",
        "same_price_hotel_df = same_price_hotel_df.merge(hotel_count, how = 'inner',\n",
        "                        on = 'Cost').rename(columns = {'Name':'Total_Restaurant'})\n",
        "\n",
        "#max hotels that share same price\n",
        "same_price_hotel_df.sort_values('Total_Restaurant', ascending = False)[:5]\n"
      ],
      "metadata": {
        "id": "lIhYZOljJXFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hotels which has max price\n",
        "same_price_hotel_df.sort_values('Cost', ascending = False)[:5]"
      ],
      "metadata": {
        "id": "H4DPehVHJczA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spliting the cusines and storing in list\n",
        "cuisine_value_list = hotel.Cuisines.str.split(', ')"
      ],
      "metadata": {
        "id": "Ytw7rOrTJgyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing all the cusines in a dict\n",
        "cuisine_dict = {}\n",
        "for cuisine_names in cuisine_value_list:\n",
        "    for cuisine in cuisine_names:\n",
        "        if (cuisine in cuisine_dict):\n",
        "            cuisine_dict[cuisine]+=1\n",
        "        else:\n",
        "            cuisine_dict[cuisine]=1"
      ],
      "metadata": {
        "id": "VgkYKw21Jj3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the dict to a data frame\n",
        "cuisine_df=pd.DataFrame.from_dict([cuisine_dict]).transpose().reset_index().rename(\n",
        "    columns={'index':'Cuisine',0:'Number of Restaurants'})"
      ],
      "metadata": {
        "id": "j-uE9VNoJnLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 cuisine\n",
        "cuisine_df.sort_values('Number of Restaurants', ascending =False)[:5]"
      ],
      "metadata": {
        "id": "EoYZl1kNJqSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spliting the cusines and storing in list\n",
        "Collections_value_list = hotel.Collections.dropna().str.split(', ')"
      ],
      "metadata": {
        "id": "sKZX3io3JuXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing all the cusines in a dict\n",
        "Collections_dict = {}\n",
        "for collection in Collections_value_list:\n",
        "    for col_name in collection:\n",
        "        if (col_name in Collections_dict):\n",
        "            Collections_dict[col_name]+=1\n",
        "        else:\n",
        "            Collections_dict[col_name]=1"
      ],
      "metadata": {
        "id": "VHaeOdt0Jxij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the dict to a data frame\n",
        "Collections_df=pd.DataFrame.from_dict([Collections_dict]).transpose().reset_index().rename(\n",
        "    columns={'index':'Tags',0:'Number of Restaurants'})"
      ],
      "metadata": {
        "id": "wkJY81PKJ01C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 collection\n",
        "Collections_df.sort_values('Number of Restaurants', ascending =False)[:5]"
      ],
      "metadata": {
        "id": "cHXH9KWmJ4HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reviews"
      ],
      "metadata": {
        "id": "f4vW_p02J7zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#in order to change data type for rating checking values\n",
        "review.Rating.value_counts()"
      ],
      "metadata": {
        "id": "2mP1y27VJ9Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing data type for each rating since had value as interger surrounded by inverted comma\n",
        "#since there is one rating as like converting it to 0 since no rating is 0 then to median\n",
        "review.loc[review['Rating'] == 'Like'] = 0\n",
        "#changing data type for rating in review data\n",
        "review['Rating'] = review['Rating'].astype('float')"
      ],
      "metadata": {
        "id": "gj21eodfKA9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#since there is one rating as like converting it to median\n",
        "review.loc[review['Rating'] == 0] = review.Rating.median()"
      ],
      "metadata": {
        "id": "H5FUpdVhKE-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing date and extracting few features for manipulation\n",
        "\n",
        "# Step 1: Split 'Metadata' by comma to separate 'Reviews' and 'Followers' parts\n",
        "# This will result in a Series of lists, e.g., ['50 Reviews', ' 100 Followers']\n",
        "metadata_split = review['Metadata'].str.split(',')\n",
        "\n",
        "# Step 2: Access the first element (Total Reviews part) and second element (Followers part)\n",
        "# Then, extract the numeric part and convert to numeric.\n",
        "review['Reviewer_Total_Review'] = pd.to_numeric(metadata_split.str[0].str.split(' ').str[0])\n",
        "review['Reviewer_Followers'] = pd.to_numeric(metadata_split.str[1].str.split(' ').str[1])\n",
        "\n",
        "# Step 3: Convert 'Time' to datetime and extract year, month, hour\n",
        "review['Time'] = pd.to_datetime(review['Time'], errors='coerce') # Add errors='coerce'\n",
        "\n",
        "# After this, you should handle the NaT values.\n",
        "# You can drop them if 'Time' is critical and you need valid dates for every row:\n",
        "review.dropna(subset=['Time'], inplace=True)\n",
        "\n",
        "# Or, fill them with a placeholder date if you want to retain all rows:\n",
        "# review['Time'].fillna(pd.Timestamp('2000-01-01 00:00:00'), inplace=True)\n",
        "\n",
        "\n",
        "review['Review_Year'] = pd.DatetimeIndex(review['Time']).year\n",
        "review['Review_Month'] = pd.DatetimeIndex(review['Time']).month\n",
        "review['Review_Hour'] = pd.DatetimeIndex(review['Time']).hour"
      ],
      "metadata": {
        "id": "qt0gMN8QKIpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Average engagement of restaurants\n",
        "avg_hotel_rating = review.groupby('Restaurant').agg({'Rating':'mean',\n",
        "        'Reviewer': 'count'}).reset_index().rename(columns = {'Reviewer': 'Total_Review'})\n",
        "avg_hotel_rating"
      ],
      "metadata": {
        "id": "-qZcQRqNK3h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data\n",
        "review[review['Restaurant'] == 4.0]"
      ],
      "metadata": {
        "id": "hnckl2rfK96j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking hotel count as total hotel in restaurant data was 105\n",
        "review.Restaurant.nunique()"
      ],
      "metadata": {
        "id": "7-guJBNVLSOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finding hotel without review\n",
        "hotel_without_review = [name for name in hotel.Name.unique().tolist()\n",
        "       if name not in review.Restaurant.unique().tolist()]\n",
        "hotel_without_review"
      ],
      "metadata": {
        "id": "3oxn9ClTLXB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 most engaging or rated restaurant\n",
        "avg_hotel_rating.sort_values('Rating', ascending = False)[:5]"
      ],
      "metadata": {
        "id": "AasjvW0xLazs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top 5 lowest rated restaurant\n",
        "avg_hotel_rating.sort_values('Rating', ascending = True)[:5]"
      ],
      "metadata": {
        "id": "NQroDcFoLiX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the most followed critic\n",
        "most_followed_reviewer = review.groupby('Reviewer').agg({'Reviewer_Total_Review':'max',\n",
        "      'Reviewer_Followers':'max', 'Rating':'mean'}).reset_index().rename(columns = {\n",
        "          'Rating':'Average_Rating_Given'}).sort_values('Reviewer_Followers', ascending = False)\n",
        "most_followed_reviewer[:5]"
      ],
      "metadata": {
        "id": "C3OcKWUGLl2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finding which year show maximum engagement\n",
        "hotel_year = review.groupby('Review_Year')['Restaurant'].apply(lambda x: x.tolist()).reset_index()\n",
        "hotel_year['Count']= hotel_year['Restaurant'].apply(lambda x: len(x))\n",
        "hotel_year"
      ],
      "metadata": {
        "id": "TXeHej83LpdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merging both data frame\n",
        "hotel = hotel.rename(columns = {'Name':'Restaurant'})\n",
        "merged = hotel.merge(review, on = 'Restaurant')\n",
        "merged.shape"
      ],
      "metadata": {
        "id": "7VxSevkKLu3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Price point of restaurants\n",
        "price_point = merged.groupby('Restaurant').agg({'Rating':'mean',\n",
        "        'Cost': 'mean'}).reset_index().rename(columns = {'Cost': 'Price_Point'})"
      ],
      "metadata": {
        "id": "hwQygxjOL0sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#price point for high rated restaurants\n",
        "price_point.sort_values('Rating',ascending = False)[:5]"
      ],
      "metadata": {
        "id": "alvtkRPiL3xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#price point for lowest rated restaurants\n",
        "price_point.sort_values('Rating',ascending = True)[:5]"
      ],
      "metadata": {
        "id": "6n21mrznL7bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rating count by reviewer\n",
        "rating_count_df = pd.DataFrame(review.groupby('Reviewer').size(), columns=[\n",
        "                                                                \"Rating_Count\"])\n",
        "rating_count_df.sort_values('Rating_Count', ascending = False)[:5]"
      ],
      "metadata": {
        "id": "VB1fh4zUL--n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Initial Data Copying\n",
        "* hotel = hotel_df.copy() and review = review_df.copy(): Created working copies of the original hotel_df and review_df to avoid modifying the originals.\n",
        "\n",
        "2. Cost Column Cleaning & Analysis (on hotel DataFrame)\n",
        "* Removed Commas and Converted to Integer: hotel['Cost'] = hotel['Cost'].str.replace(\",\",\"\").astype('int64'). This crucial step converted the 'Cost' column from a string (object) type, where costs were represented with commas (e.g., '1,300'), into a numeric (integer) type, enabling calculations.\n",
        "\n",
        "* Identified Top 5 Costliest & Economy Restaurants: Sorted the hotel DataFrame by 'Cost' to list the most expensive and most economical restaurants.\n",
        "\n",
        "* Grouped Hotels by Same Price Point:\n",
        "\n",
        "  * Created hotel_dict to store restaurant names grouped by their cost.\n",
        "\n",
        "  * Converted hotel_dict into same_price_hotel_df DataFrame.\n",
        "\n",
        "  * Counted total restaurants per cost using hotel.groupby('Cost')['Name'].count().\n",
        "\n",
        "  * Merged this count back into same_price_hotel_df to show Total_Restaurant at each price point.\n",
        "\n",
        "* Identified Price Points with Max Hotels: Sorted same_price_hotel_df to find the costs shared by the highest number of restaurants.\n",
        "\n",
        "* Identified Hotels with Max Price: Sorted same_price_hotel_df to find the highest price points and the restaurants at those prices.\n",
        "\n",
        "3. Cuisines Analysis (on hotel DataFrame)\n",
        "* Split Cuisines: hotel.Cuisines.str.split(', ') separated multiple cuisines listed in a single string into individual cuisine entries.\n",
        "\n",
        "* Counted Cuisines: Iterated through the split list to populate cuisine_dict with counts of each unique cuisine.\n",
        "\n",
        "* Created cuisine_df: Converted cuisine_dict into a DataFrame to easily view cuisine counts.\n",
        "\n",
        "* Identified Top 5 Cuisines: Sorted cuisine_df to find the most popular cuisines by the number of restaurants offering them.\n",
        "\n",
        "4. Collections Analysis (on hotel DataFrame)\n",
        "* Split Collections: hotel.Collections.dropna().str.split(', ') similar to cuisines, split multiple collections. dropna() was used here before splitting.\n",
        "\n",
        "* Counted Collections: Iterated through split collections to populate Collections_dict with counts.\n",
        "\n",
        "* Created Collections_df: Converted Collections_dict into a DataFrame.\n",
        "\n",
        "* Identified Top 5 Collections: Sorted Collections_df to find the most common restaurant collection tags.\n",
        "\n",
        "5. Rating Column Cleaning & Feature Engineering (on review DataFrame)\n",
        "* Handled 'Like' Rating: Replaced a single 'Like' rating entry with 0.\n",
        "\n",
        "* Converted Rating to Float: review['Rating'] = review['Rating'].astype('float'). This converted the 'Rating' column to a numeric (float) type.\n",
        "\n",
        "* Filled '0' Rating with Median: review.loc[review['Rating'] == 0] = review.Rating.median(). This replaced the previously set '0' rating (which stemmed from 'Like') with the median rating, providing a more central value.\n",
        "\n",
        "6. Metadata & Time Feature Engineering (on review DataFrame)\n",
        "* Extracted 'Reviewer_Total_Review' and 'Reviewer_Followers':\n",
        "\n",
        "  * Split the 'Metadata' string by commas to separate the \"Reviews\" and \"Followers\" parts.\n",
        "\n",
        "  * Extracted the numeric values from these parts (e.g., \"50 Reviews\" -> 50, \"100 Followers\" -> 100) and converted them to numeric types.\n",
        "\n",
        "* Converted 'Time' to Datetime: review['Time']=pd.to_datetime(review['Time']). This converted the 'Time' column into datetime objects.\n",
        "\n",
        "* Extracted Time-Based Features:\n",
        "\n",
        "  * Review_Year: Extracted the year from the 'Time' column.\n",
        "\n",
        "  * Review_Month: Extracted the month from the 'Time' column.\n",
        "\n",
        "  * Review_Hour: Extracted the hour from the 'Time' column.\n",
        "\n",
        "7. Reviewer Engagement & Restaurant Rating Analysis (on review DataFrame)\n",
        "* Calculated Average Hotel Rating & Total Reviews: Grouped review by 'Restaurant' to find the average 'Rating' and total Reviewer count (Total_Review) for each restaurant.\n",
        "\n",
        "* Identified Hotels Without Reviews: Compared unique 'Restaurant' names in hotel and review to find restaurants that exist in hotel but have no corresponding reviews.\n",
        "\n",
        "* Identified Top 5 Most & Lowest Rated Restaurants: Sorted avg_hotel_rating by 'Rating' to find restaurants with the highest and lowest average ratings.\n",
        "\n",
        "* Found Most Followed Critic: Grouped by 'Reviewer' to find the maximum 'Reviewer_Total_Review', 'Reviewer_Followers', and average 'Rating_Given' for each reviewer, then sorted to find the most followed.\n",
        "\n",
        "* Analyzed Engagement by Year: Grouped by Review_Year to list restaurants reviewed in each year and count the total reviews for that year. (Note: The output shows an anomaly with 1970.00 and 4.0, which seems like leftover invalid data from previous errors).\n",
        "\n",
        "8. Merging hotel and review DataFrames\n",
        "* hotel = hotel.rename(columns = {'Name':'Restaurant'}): Renamed 'Name' in hotel to 'Restaurant' to match the review DataFrame for merging.\n",
        "\n",
        "* merged = hotel.merge(review, on = 'Restaurant'): Performed an inner merge on the 'Restaurant' column to combine the hotel and review information into a single merged DataFrame. The shape (9999, 17) indicates the number of combined review entries.\n",
        "\n",
        "9. Price Point Analysis of Restaurants (on merged DataFrame)\n",
        "* Calculated Price Point: Grouped merged by 'Restaurant' to find the average 'Rating' and 'Cost' (renamed to 'Price_Point').\n",
        "\n",
        "* Price Point for High/Lowest Rated Restaurants: Sorted price_point by 'Rating' to see the average cost of highly-rated and lowest-rated restaurants.\n",
        "\n",
        "10. Rating Count by Reviewer (on review DataFrame)\n",
        "* Calculated Rating Count per Reviewer: Grouped review by 'Reviewer' and counted the size (number of ratings) for each.\n",
        "\n",
        "* Identified Top 5 Reviewers by Rating Count: Sorted rating_count_df to find reviewers who have given the most ratings."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1: Distribution of Ratings"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd # Ensure pandas is imported if not already\n",
        "\n",
        "# Chart - 1 visualization code\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(merged['Rating'], bins=10, kde=True, color='skyblue')\n",
        "plt.title('Distribution of Restaurant Ratings')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nRating Value Counts:\")\n",
        "print(merged['Rating'].value_counts().sort_index(ascending=False))"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart (generated by sns.histplot(df['Rating'], bins=10, kde=True, color='skyblue')) provides insights into how restaurant ratings are distributed across your dataset."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Skewness: The distribution is likely left-skewed (or negatively skewed), meaning the tail is longer on the lower side and the majority of ratings are concentrated towards the higher end of the scale.\n",
        "\n",
        "2. Peak Rating: You'll likely observe a prominent peak (mode) at 4.0, 4.5, or 5.0. This indicates that restaurants generally receive high ratings in your dataset.\n",
        "\n",
        "3. Frequency of High Ratings: A large number of reviews fall into the 3.5 to 5.0 range, suggesting overall customer satisfaction is high or that extreme negative experiences are less frequently reviewed or recorded.\n",
        "\n",
        "4. Lower Ratings: There will be fewer reviews in the lower rating ranges (e.g., below 3.0), but their presence indicates some level of dissatisfaction exists.\n",
        "\n",
        "5. Rating Scale Utilization: The chart shows how uniformly (or non-uniformly) the entire rating scale (likely 0.5 to 5.0) is utilized by reviewers."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the gained insights can significantly help create a positive business impact for a Zomato-like platform or restaurants themselves.\n",
        "\n",
        "\n",
        "*Positive Business Impact:*\n",
        "\n",
        "Several insights can drive positive growth:\n",
        "\n",
        "1. High Average Ratings (from \"Distribution of Restaurant Ratings\"):\n",
        "\n",
        "* Impact: A high concentration of positive ratings (e.g., 4.0-5.0) signifies strong overall customer satisfaction. This builds trust for new users Browse the platform, encouraging them to try highly-rated restaurants. For restaurants, it validates their quality and service.\n",
        "\n",
        "* Actionable Insight: Zomato can leverage this by prominently featuring highly-rated restaurants, creating \"Best Rated\" collections, and using positive aggregate scores in marketing. Restaurants can highlight their high ratings to attract more customers.\n",
        "\n",
        "2. Top Cuisines Identification (from \"Top 10 Most Common Cuisines\"):\n",
        "\n",
        "* Impact: Knowing the most popular cuisines (e.g., North Indian, Fast Food) allows the platform to tailor its offerings and marketing strategies.\n",
        "\n",
        "* Actionable Insight: Zomato can invest in onboarding more restaurants offering these popular cuisines in underserved areas, or run targeted promotions for them. Restaurants can focus on perfecting these popular dishes or adding them to their menu if feasible.\n",
        "\n",
        "3. Cost vs. Rating Relationship (from Hypothesis 1):\n",
        "\n",
        "* Impact: If Hypothesis 1 (High-cost restaurants have higher ratings) is supported, it suggests customers perceive higher value or quality with higher prices. This validates premium pricing strategies.\n",
        "\n",
        "* Actionable Insight: Zomato can create \"Fine Dining\" or \"Premium Experience\" collections for high-cost, high-rated restaurants. Restaurants can justify their pricing by ensuring a premium experience that aligns with customer expectations.\n",
        "\n",
        "4. Reviewer Activity and Consistency (from Hypothesis 3):\n",
        "\n",
        "* Impact: If high-activity reviewers are found to be more consistent (less variance in ratings), their reviews might be considered more reliable or 'expert' opinions.\n",
        "\n",
        "* Actionable Insight: Zomato can implement a \"Verified Reviewer\" or \"Top Contributor\" badge for such users, giving their reviews more weight or prominence. This builds a more credible review ecosystem."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Insights Leading to Negative Growth:*\n",
        "\n",
        "While most insights are geared towards positive impact, some can indicate areas that, if unaddressed, could lead to negative growth:\n",
        "\n",
        "1. Presence of Lower Ratings:\n",
        "\n",
        "* Reason: Although high ratings dominate, the existence of ratings like 0.5 or 1.0 (visible in the histogram's left tail) indicates customer dissatisfaction. If these lower ratings are consistently for specific restaurants or certain aspects (e.g., delivery, hygiene), it can drive customers away.\n",
        "\n",
        "* Justification: Even a small percentage of very negative experiences, if widely publicized or concentrated, can deter potential customers. A few viral negative reviews can significantly damage a restaurant's reputation or a platform's perceived reliability. Zomato's business model relies heavily on trust and positive user experiences.\n",
        "\n",
        "2. Significant Data Quality Issues (from Data Wrangling Insights):\n",
        "\n",
        "* Reason: Insights like Collections having over 50% missing values or duplicate rows with missing critical information ('American Wild Wings' and 'Arena Eleven' example) point to underlying data collection or maintenance problems.\n",
        "\n",
        "* Justification: If key restaurant information (like Collections, which categorize restaurants) is frequently missing, users struggle to find what they're looking for, leading to a poor user experience. Inaccurate or incomplete data reduces the platform's utility and reliability, potentially causing users to abandon the platform or distrust its information. Duplicates also inflate data, leading to skewed analytics and potentially showing more restaurants or reviews than truly exist, which is misleading.\n",
        "\n",
        "3. Ambiguous Rating System (e.g., 'Like', 'Meh!'):\n",
        "\n",
        "* Reason: The original 'Rating' column containing non-numerical text like 'Like', 'Meh!', 'Avoid' required significant mapping and assumptions to convert to numerical values. This ambiguity can confuse users about what their rating truly means.\n",
        "\n",
        "* Justification: An inconsistent or unclear rating system can lead to less precise feedback. If users are unsure how to rate or if their subjective \"Like\" is interpreted as a 5.0, it dilutes the granularity and reliability of the overall rating, making it harder for others to trust and for restaurants to get precise feedback for improvement.\n",
        "\n",
        "These \"negative growth\" insights aren't inherently bad if they are addressed. They highlight areas where the platform or restaurants need to improve data collection, service quality, or user experience to prevent customer churn and ensure sustainable growth."
      ],
      "metadata": {
        "id": "Gw5bf3jpa6d_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2: Distribution of Restaurant Cost"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd # Ensure pandas is imported if not already\n",
        "\n",
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(merged['Cost'], bins=30, kde=True, color='lightcoral')\n",
        "plt.title('Distribution of Restaurant Cost (Estimated)')\n",
        "plt.xlabel('Cost')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCost Statistics:\")\n",
        "print(merged['Cost'].describe())"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart, a histogram with a KDE (Kernel Density Estimate), shows how the estimated cost of dining is distributed across the restaurants in your dataset."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Skewness: The distribution of Cost is typically right-skewed (or positively skewed). This means there's a long tail extending towards higher costs, indicating that while most restaurants are in the lower to medium cost range, there are a few very expensive outliers.\n",
        "\n",
        "2. Concentration of Restaurants: The majority of restaurants (and their associated reviews) will likely fall into the lower to mid-range cost brackets. This suggests that affordable to moderately priced eateries are more prevalent in your dataset.\n",
        "\n",
        "3. Presence of High-Cost Outliers: The KDE curve and the bars on the right side of the histogram will show that there are fewer restaurants at very high costs, but they do exist, extending the range of the distribution significantly.\n",
        "\n",
        "4. Bimodality (Possible): Depending on your data, you might even observe some bimodality, indicating distinct clusters of \"budget-friendly\" and \"mid-range\" restaurants, before the long tail of high-end establishments."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, absolutely.\n",
        "\n",
        "1. Understanding Market Segments: Knowing the cost distribution helps Zomato and restaurants understand the dominant price points in the market. If most restaurants are affordable, Zomato can cater marketing to budget-conscious users or promote deals. If there's a segment of high-cost restaurants, it points to a premium market segment they can target.\n",
        "\n",
        "2. Pricing Strategy for Restaurants: Restaurants can compare their pricing to the overall distribution. If they're a high-cost restaurant in a predominantly low-cost area, they need to justify that cost with superior quality or experience. Conversely, if they're too cheap, they might be missing out on revenue.\n",
        "\n",
        "3. Identifying Gaps: If there's a clear gap in a certain cost range (e.g., very few mid-range options), it might indicate an unserved market niche for new restaurants or for Zomato to actively onboard."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If certain conditions or interpretations arise from this chart, they could lead to negative growth if not addressed:\n",
        "\n",
        "1. Over-saturation in a Specific Cost Segment:\n",
        "\n",
        "* Reason: If the chart shows an extreme peak in a very narrow cost range (e.g., almost all restaurants are within a tight \"mid-cost\" bracket), it suggests fierce competition within that segment.\n",
        "\n",
        "* Justification: For a restaurant, being just another \"me-too\" option in an oversaturated market makes it harder to stand out, attract customers, and maintain profitability, potentially leading to stagnation or decline. For Zomato, if their platform offers too many similar options without clear differentiation, users might struggle with choice overload or not find unique dining experiences, potentially reducing engagement over time.\n",
        "\n",
        "2. Lack of Restaurants at High-Demand Price Points:\n",
        "\n",
        "* Reason: If the distribution shows very few restaurants in a cost range that is actually in high demand (e.g., users are searching for affordable options, but most restaurants listed are expensive), it indicates a mismatch between supply and demand on the platform.\n",
        "\n",
        "* Justification: If Zomato fails to provide enough options at price points consumers are actively seeking, users might abandon the platform in favor of competitors that offer more relevant choices, leading to reduced traffic and engagement for Zomato and missed business for restaurants that could fill that gap."
      ],
      "metadata": {
        "id": "FBUSt0uacpRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3: Top 10 Most Common Cuisines"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd # Ensure pandas is imported if not already\n",
        "\n",
        "# Chart - 3 visualization code\n",
        "# Explode cuisines if they are comma-separated and count top ones\n",
        "# Assuming 'Cuisines' column might contain multiple cuisines separated by commas\n",
        "cuisines_flat = merged['Cuisines'].str.split(', ').explode().str.strip()\n",
        "top_cuisines = cuisines_flat.value_counts().head(10)\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(x=top_cuisines.index, y=top_cuisines.values, palette='viridis')\n",
        "plt.title('Top 10 Most Common Cuisines')\n",
        "plt.xlabel('Cuisine')\n",
        "plt.ylabel('Number of Restaurants/Reviews')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 10 Most Common Cuisines:\")\n",
        "print(top_cuisines)"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bar plot displays the ten most frequently occurring cuisines in your dataset, indicating their popularity or prevalence among the listed restaurants and reviews."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Dominant Cuisines: You will clearly see which cuisines are the most frequently offered or reviewed. Typically, this might be cuisines like 'North Indian', 'Chinese', 'Fast Food', 'South Indian', etc., depending on the geographical focus of the dataset (e.g., if it's Hyderabad data, you might see 'Biryani' or 'Andhra' prominent).\n",
        "\n",
        "2. Market Saturation: The heights of the bars provide a visual representation of how saturated the market might be with certain types of cuisine. A very tall bar means many restaurants offer that cuisine.\n",
        "\n",
        "3. Cuisine Diversity: While it shows the top 10, it gives an indirect sense of diversity. If the top few bars are disproportionately taller than the others, it suggests less diversity at the very top."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, these insights are highly valuable for positive business impact:\n",
        "\n",
        "1. Strategic Expansion for Zomato: Knowing the most common cuisines helps Zomato identify which types of restaurants are plentiful. They can focus efforts on deepening market penetration in these popular segments (e.g., more precise filtering, specific collection curation). They can also identify underrepresented cuisines that might have emerging demand and actively onboard restaurants offering those.\n",
        "\n",
        "2. Restaurant Menu Planning: Existing restaurants can analyze if their offerings align with popular demand. New restaurants can use this to gauge competition. If a cuisine is highly popular but still has growth potential, it's a viable market for new ventures.\n",
        "\n",
        "3. Targeted Marketing: Zomato can create cuisine-specific marketing campaigns (e.g., \"Craving North Indian? Explore these top spots!\"). Restaurants can emphasize their most popular cuisine.\n",
        "\n",
        "4. Improved User Experience: By understanding what users are most likely searching for, Zomato can optimize its search filters, recommendations, and curated lists, making it easier for users to find what they want, leading to higher engagement and satisfaction."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These insights can also point to potential pitfalls that, if not addressed, could lead to negative growth:\n",
        "\n",
        "1. Over-Saturation and Intense Competition:\n",
        "\n",
        "* Reason: If one or two cuisines are overwhelmingly dominant (very tall bars compared to others), it suggests a highly competitive market for those specific cuisines.\n",
        "\n",
        "* Justification: For a new restaurant entering that market, it would be extremely difficult to gain visibility and customers against so many established players, potentially leading to quick failure. For Zomato, if they solely focus on these saturated cuisines, they might fail to offer unique options, leading to user fatigue or driving users to competitor platforms that provide more diverse choices or better niche offerings. It also implies that existing restaurants in these categories face immense pressure to differentiate, potentially leading to price wars that erode profitability.\n",
        "\n",
        "2. Missed Opportunities/Stagnation:\n",
        "\n",
        "* Reason: If the chart shows a long tail of very few restaurants for certain niche or emerging cuisines that might have growing consumer interest (which this chart might not explicitly show, but implies by showing only top 10), it indicates unfulfilled demand.\n",
        "\n",
        "* Justification: Zomato might experience negative growth if it fails to adapt and bring in new, trending, or underserved cuisine types. If users consistently can't find specific, desired cuisines on the platform, they might migrate to competitors, leading to a loss of market share and potentially a perception of Zomato being \"behind the curve\" in catering to evolving tastes. This stagnation in offerings can impact user acquisition and retention."
      ],
      "metadata": {
        "id": "ezcrOC1qeCKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6: Ratings Over Time"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Chart - 6 visualization code (Updated to use 'merged' DataFrame)\n",
        "\n",
        "# Ensure 'Time' column is datetime\n",
        "# This check is largely a safeguard; the comprehensive wrangling code already handles this.\n",
        "if not pd.api.types.is_datetime64_any_dtype(merged['Time']):\n",
        "    print(\"Warning: 'Time' column is not in datetime format. Attempting conversion.\")\n",
        "    # Use the same robust conversion as in wrangling\n",
        "    merged['Time'] = pd.to_datetime(merged['Time'], errors='coerce')\n",
        "    merged.dropna(subset=['Time'], inplace=True) # Drop rows where conversion failed\n",
        "    if merged['Time'].empty:\n",
        "        print(\"Error: 'Time' column is empty after conversion. Cannot plot ratings over time.\")\n",
        "        exit()\n",
        "\n",
        "# Aggregate average rating per month/year\n",
        "merged['YearMonth'] = merged['Time'].dt.to_period('M')\n",
        "avg_rating_over_time = merged.groupby('YearMonth')['Rating'].mean().reset_index()\n",
        "avg_rating_over_time['YearMonth'] = avg_rating_over_time['YearMonth'].astype(str) # Convert Period to string for plotting\n",
        "\n",
        "# Sort by YearMonth for proper chronological order on the plot\n",
        "avg_rating_over_time = avg_rating_over_time.sort_values('YearMonth')\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "sns.lineplot(x='YearMonth', y='Rating', data=avg_rating_over_time, marker='o', color='purple')\n",
        "plt.title('Average Rating Over Time')\n",
        "plt.xlabel('Year-Month')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(True, alpha=0.75)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nFirst 5 and Last 5 Average Ratings Over Time:\")\n",
        "print(avg_rating_over_time.head())\n",
        "print(avg_rating_over_time.tail())"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line plot shows how the average restaurant rating changes month by month (or year by year) across the entire dataset. It provides insights into temporal trends in customer satisfaction."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Trends (Upward/Downward/Stable): The most immediate insight is whether the average rating is increasing, decreasing, or remaining relatively stable over the observed period.\n",
        "\n",
        "* An upward trend suggests improving overall customer satisfaction or perhaps the growth of higher-quality restaurants on the platform.\n",
        "\n",
        "* A downward trend could signal declining satisfaction, emergence of lower-quality options, or issues with service over time.\n",
        "\n",
        "* A stable trend indicates consistent performance across the period.\n",
        "\n",
        "2. Seasonality/Fluctuations: Look for repeating peaks and troughs at regular intervals (e.g., every year around the same months). This could indicate seasonal variations in dining habits, tourism, or even operational challenges for restaurants during specific periods (e.g., holidays, festivals).\n",
        "\n",
        "3. Specific Events: Sharp, sudden drops or spikes that aren't seasonal might correlate with specific events (e.g., a major food festival leading to high ratings, or a public health concern affecting dining out leading to lower ratings for some places)."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding rating trends over time is crucial for proactive management and strategic planning, leading to significant positive business impact:\n",
        "\n",
        "1. Performance Monitoring & Quality Assurance:\n",
        "\n",
        "* Actionable Impact: For Zomato, monitoring a downward trend in overall ratings acts as an early warning system. It could prompt investigations into issues like declining service standards among partners, changes in user expectations, or increased competition affecting quality. Conversely, an upward trend validates ongoing efforts to improve restaurant quality or user experience. This allows Zomato to support restaurants in improving quality or to highlight positive platform-wide improvements.\n",
        "\n",
        "2. Targeted Interventions: If specific months consistently show lower ratings (e.g., during summer heatwaves in Hyderabad affecting outdoor dining), Zomato could advise restaurants on offering seasonal incentives (e.g., special offers for AC-equipped restaurants, indoor dining promotions).\n",
        "\n",
        "* Actionable Impact: This proactive guidance helps restaurants maintain customer satisfaction even during challenging periods, ultimately benefiting Zomato's ecosystem by keeping reviews positive.\n",
        "\n",
        "3. Marketing & Promotions: Identifying periods of high average ratings can inform marketing strategies.\n",
        "\n",
        "* Actionable Impact: Zomato could launch \"Best of the Season\" campaigns during periods of consistently high ratings, leveraging positive sentiment to attract more users."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certain observations from this chart can indicate potential drivers of negative growth if they remain unaddressed:\n",
        "\n",
        "1. Sustained Decline in Average Ratings:\n",
        "\n",
        "* Reason: A consistent and significant downward trend in the average rating over an extended period.\n",
        "\n",
        "* Justification: This is a direct indicator of decreasing customer satisfaction with the overall restaurant offerings or potentially Zomato's service (e.g., delivery quality). If customers consistently have worse experiences, they will reduce their usage of the platform, choose competitors, or stop dining out as frequently, directly leading to a decline in transactions and user base for Zomato. It signifies a systemic problem that could severely impact reputation and revenue.\n",
        "\n",
        "2. Recurring Seasonal Dips Not Addressed:\n",
        "\n",
        "* Reason: If the chart shows predictable, significant dips in ratings during specific months (e.g., every monsoon season in Hyderabad, specific festival periods) that are not being mitigated.\n",
        "\n",
        "* Justification: These recurring dips represent predictable periods of customer dissatisfaction or operational stress for restaurants. If Zomato and its partners don't develop strategies to counteract these issues (e.g., better rain-proof delivery, special services during busy holidays), these periods will consistently lead to negative user experiences and reduced engagement, contributing to a cyclical pattern of negative growth during those times of the year. It shows a failure to adapt to known challenges."
      ],
      "metadata": {
        "id": "OHTNDs5vjO04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7: Top 10 Restaurants by Average Rating (and their Review Count)"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd # Ensure pandas is imported if not already\n",
        "\n",
        "# Calculate average rating and review count per restaurant\n",
        "restaurant_summary = merged.groupby('Restaurant').agg(\n",
        "    Average_Rating=('Rating', 'mean'),\n",
        "    Review_Count=('Review', 'count')\n",
        ").reset_index()\n",
        "\n",
        "# Filter for restaurants with a decent number of reviews to avoid bias from few reviews\n",
        "min_reviews_threshold = 10 # Adjust this threshold as needed\n",
        "top_rated_restaurants = restaurant_summary[restaurant_summary['Review_Count'] >= min_reviews_threshold].sort_values(by='Average_Rating', ascending=False).head(10)\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(x='Average_Rating', y='Restaurant', data=top_rated_restaurants, palette='magma')\n",
        "plt.title(f'Top 10 Restaurants by Average Rating (Min {min_reviews_threshold} Reviews)')\n",
        "plt.xlabel('Average Rating')\n",
        "plt.ylabel('Restaurant')\n",
        "plt.grid(axis='x', alpha=0.75)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 10 Restaurants by Average Rating:\")\n",
        "print(top_rated_restaurants)"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bar plot highlights the restaurants that have the highest average ratings, considering only those with a minimum number of reviews to ensure statistical significance."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Which specific restaurants are performing exceptionally well?\n",
        "\n",
        "* Observation: The bars directly show the names and average ratings of the top performers.\n",
        "\n",
        "* Impact: This is highly actionable. Zomato can prominently feature these restaurants in \"Editor's Choice,\" \"Top Rated,\" or \"Must-Visit\" collections. This drives traffic to these high-performing partners, strengthens relationships, and ensures users discover top-quality dining experiences.\n",
        "\n",
        "2. Are these top restaurants consistently rated, or do they have very few reviews?\n",
        "\n",
        "* Observation: The min_reviews_threshold helps mitigate the \"few reviews, high rating\" bias. The Review_Count included in the print output provides this detail.\n",
        "\n",
        "* Impact: High ratings backed by a substantial number of reviews are more credible and trustworthy. Zomato can emphasize this credibility, reassuring users that these ratings are reliable, which builds platform trust. If a restaurant shows high average ratings with few reviews, it's a positive signal, but needs more data to be fully trustworthy.\n",
        "\n",
        "3. Do these top restaurants share common characteristics (e.g., cuisine, cost, location)?\n",
        "\n",
        "* Observation: This requires cross-referencing with other data points (not directly from this chart, but using the insights gained).\n",
        "\n",
        "* Impact: If a pattern emerges (e.g., most top-rated restaurants are fine-dining Italian or specific chains), Zomato can gain insights into what truly drives excellence in their ecosystem. This can inform onboarding strategies (seeking more similar restaurants), marketing campaigns (targeting users who prefer these types of establishments), and even provide benchmarks for other restaurants aiming to improve."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "\n",
        "1. Actionable Insight: Directly highlights top-performing restaurants. Zomato can feature these restaurants prominently (e.g., \"Editor's Choice,\" \"Must-Visit\" collections, prime ad slots). This helps users discover quality, builds trust in Zomato's recommendations, and strengthens relationships with high-quality partners. This visibility drives more traffic and orders to these restaurants and, by extension, to Zomato."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights Leading to Negative Growth:\n",
        "\n",
        "1. Potential Over-Reliance on Few Reviews: While we filtered for a minimum, if some \"top\" restaurants still have relatively few reviews, their high average might not be truly representative.\n",
        "\n",
        "* Justification: Promoting restaurants based on insufficient data could lead to user disappointment if their experience doesn't match the inflated rating. This erodes trust in Zomato's rating system and can cause users to look for more reliable sources of information, hindering platform growth."
      ],
      "metadata": {
        "id": "2l1LLNYBly5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9: Rating Distribution by Collection Presence"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd # Ensure pandas is imported if not already\n",
        "\n",
        "# Chart - 9 visualization code (Updated to use 'merged' DataFrame)\n",
        "\n",
        "# Example: Check if restaurant is part of *any* collection (excluding 'Not Available')\n",
        "merged['In_Collection'] = merged['Collections'].apply(lambda x: 'Yes' if x != 'Not Available' else 'No')\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='In_Collection', y='Rating', data=merged, palette='cividis')\n",
        "plt.title('Rating Distribution: In Collection vs. Not In Collection')\n",
        "plt.xlabel('Is Restaurant in a Zomato Collection?')\n",
        "plt.ylabel('Rating')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nMedian Rating: In Collection vs. Not In Collection:\")\n",
        "print(merged.groupby('In_Collection')['Rating'].median())"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This box plot compares the distribution of ratings for restaurants that are part of a Zomato \"Collection\" (e.g., \"Must Try,\" \"Great Breakfast Spots\") versus those that are not."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Do restaurants in Zomato Collections generally have higher ratings?\n",
        "\n",
        "* Observation: Compare the median rating (middle line) of the \"Yes\" (In Collection) box to the \"No\" (Not In Collection) box.\n",
        "\n",
        "* Impact: If \"In Collection\" restaurants consistently show higher median ratings, it validates Zomato's curation efforts. It demonstrates that collections effectively highlight quality restaurants, which builds user trust in Zomato's recommendations. This encourages users to explore collections, leading to higher engagement and satisfaction.\n",
        "\n",
        "2. Are ratings for \"In Collection\" restaurants more consistent (less variance)?\n",
        "\n",
        "* Observation: Compare the height of the boxes (IQR) for \"Yes\" vs. \"No\". A shorter box implies more consistent ratings.\n",
        "\n",
        "* Impact: If collection restaurants have tighter rating distributions, it further reinforces their reliability. This helps Zomato ensure a consistent quality experience for users who rely on curated lists, reducing the risk of disappointment.\n",
        "\n",
        "3. Are there many highly-rated restaurants not in collections?\n",
        "\n",
        "* Observation: While not directly shown, if the \"Not In Collection\" box still has a relatively high median and plenty of high-rating outliers, it suggests there are undiscovered gems.\n",
        "\n",
        "* Impact: This points to an opportunity for Zomato's curation team to expand and refresh collections by identifying and adding these high-performing, currently unfeatured restaurants. This can continuously enrich the platform's content and provide fresh recommendations to users, preventing stagnation."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "\n",
        "1. Validation of Curation: If restaurants \"In Collection\" consistently have higher median ratings and/or tighter rating distributions.\n",
        "\n",
        "2. Actionable Impact: This validates Zomato's curation strategy, showing that collections effectively highlight quality establishments. Users can trust these curated lists, leading to higher satisfaction and repeated use of the collection feature. This reinforces Zomato's role as a trusted guide in dining, fostering user loyalty."
      ],
      "metadata": {
        "id": "gkR3Cs7mq30G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights Leading to Negative Growth:\n",
        "\n",
        "1. No Difference or Lower Quality in Collections: If the chart shows no significant difference in ratings, or even worse, if \"In Collection\" restaurants have lower median ratings or higher variance.\n",
        "\n",
        "* Justification: This means Zomato's collections are failing to effectively curate higher-quality restaurants. Users relying on these collections might experience disappointment, leading to:\n",
        "\n",
        "2. Distrust in Recommendations: Users will lose faith in Zomato's curated lists.\n",
        "\n",
        "3. Reduced Engagement: Users will stop using collections and might even reduce overall platform usage.\n",
        "\n",
        "4. Reputational Damage: Zomato's image as a reliable dining guide could be negatively impacted, leading to user churn and hindering growth."
      ],
      "metadata": {
        "id": "ahmrhtCGnJOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10: Average Rating of Top N Cuisines"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Explode cuisines to get a long format DataFrame suitable for grouping by individual cuisine\n",
        "# This creates a new Series where each cuisine gets its own row, preserving the original index.\n",
        "cuisines_exploded = merged['Cuisines'].str.split(', ').explode().str.strip()\n",
        "\n",
        "# Now, create a temporary DataFrame that pairs each exploded cuisine with its corresponding rating\n",
        "# We use .loc to ensure the index alignment is correct, even if merged has duplicates or issues.\n",
        "# This approach effectively creates (cuisine, rating) pairs for every single cuisine instance.\n",
        "temp_df = pd.DataFrame({\n",
        "    'Cuisine': cuisines_exploded,\n",
        "    'Rating': merged['Rating'].loc[cuisines_exploded.index] # Align Ratings back to exploded cuisines\n",
        "})\n",
        "\n",
        "# Calculate average rating per cuisine using this temporary DataFrame\n",
        "cuisine_ratings = temp_df.groupby('Cuisine')['Rating'].mean().reset_index()\n",
        "\n",
        "\n",
        "# Get the top 10 most common cuisines first (as identified in previous charts, e.g., Chart 3)\n",
        "# This part of the logic is correct for identifying top cuisines\n",
        "top_10_common_cuisines_names = merged['Cuisines'].str.split(', ').explode().str.strip().value_counts().head(10).index.tolist()\n",
        "\n",
        "# Filter average ratings for only these top 10 cuisines\n",
        "avg_rating_top_10_cuisines = cuisine_ratings[cuisine_ratings['Cuisine'].isin(top_10_common_cuisines_names)]\n",
        "avg_rating_top_10_cuisines = avg_rating_top_10_cuisines.sort_values(by='Rating', ascending=False)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(x='Rating', y='Cuisine', data=avg_rating_top_10_cuisines, palette='crest')\n",
        "plt.title('Average Rating of Top 10 Most Common Cuisines')\n",
        "plt.xlabel('Average Rating')\n",
        "plt.ylabel('Cuisine')\n",
        "plt.grid(axis='x', alpha=0.75)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nAverage Rating of Top 10 Most Common Cuisines:\")\n",
        "print(avg_rating_top_10_cuisines)"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This moves beyond just popularity to assess satisfaction. It directly compares the average ratings of the most common cuisines, helping identify which popular cuisines genuinely satisfy customers."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Satisfaction by Cuisine Type: Clearly shows which of the popular cuisines receive the highest average ratings, indicating high customer satisfaction. Conversely, it highlights popular cuisines with lower average ratings.\n",
        "\n",
        "2. Quality vs. Quantity: Provides a reality check: a cuisine can be common but not necessarily highly rated."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impact:\n",
        "\n",
        "1. Targeted Improvement/Promotion: Zomato can promote highly-rated popular cuisines to attract more users. For restaurants, it identifies areas for improvement: if a common cuisine has a low average rating, it's a signal to improve quality or service.\n",
        "\n",
        "2. Investment Guidance: Zomato might prioritize onboarding more restaurants of highly-rated popular cuisines, or even invest in programs to help lower-rated popular cuisines improve."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights Leading to Negative Growth:\n",
        "\n",
        "1. Promoting Low-Quality Popular Cuisines: If Zomato heavily promotes a popular cuisine that consistently receives low average ratings.\n",
        "\n",
        "2. Justification: Users will experience disappointment, leading to distrust in Zomato's recommendations and a negative perception of the platform's ability to guide them to quality dining, directly impacting user retention."
      ],
      "metadata": {
        "id": "VhjRcSlosLzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Correlation Heatmap visualization code (Updated to use 'merged' DataFrame)\n",
        "\n",
        "# Select only numerical columns for the correlation matrix from 'merged'\n",
        "numerical_df = merged.select_dtypes(include=['number'])\n",
        "\n",
        "# Drop columns that might be all NaN or have no variance, which would cause errors\n",
        "# Also, exclude identifier-like columns that are numeric but not meaningful for correlation\n",
        "# 'Restaurant_ID' is in 'hotel_df' before merge, 'Reviewer_ID' is in 'review_df'.\n",
        "# Let's ensure these are handled correctly if they end up as numeric in merged.\n",
        "# Based on your prior code, these might not be directly in 'merged' with these exact names\n",
        "# or might have been excluded/renamed. This line is a safe guard.\n",
        "numerical_df = numerical_df.drop(columns=['Restaurant_ID', 'Reviewer_ID'], errors='ignore')\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = numerical_df.corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
        "plt.title('Correlation Heatmap of Numerical Features')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCorrelation Matrix (Numerical Features):\\n\", correlation_matrix)"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation heatmap visually represents the pairwise correlation coefficients between all numerical columns in your DataFrame. It helps to quickly identify strong positive or negative linear relationships between variables, which is crucial for understanding underlying data structures and for feature selection in machine learning."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the correlation heatmap, you can identify:\n",
        "\n",
        "Strong Positive Correlations (Warm Colors, close to +1): Look for dark red cells. This indicates that as one variable increases, the other tends to increase proportionally.\n",
        "\n",
        "Example: You might find a positive correlation between Rating and Cost (if higher cost restaurants tend to have higher ratings). Or between Number_of_Reviews and Followers (if active reviewers gain more followers).\n",
        "\n",
        "Strong Negative Correlations (Cool Colors, close to -1): Look for dark blue cells. This indicates that as one variable increases, the other tends to decrease.\n",
        "\n",
        "Example: Less common in this context, but perhaps Cost and some hypothetical 'Discount_Rate' could be negatively correlated.\n",
        "\n",
        "Weak or No Correlations (Colors close to 0, typically white/light gray): Indicates little to no linear relationship between variables.\n",
        "\n",
        "Example: Rating might have a very weak correlation with Average_Cost_for_Two if good and bad restaurants exist across all price points.\n",
        "\n",
        "Self-Correlation: The diagonal will always be 1 (perfect positive correlation) as a variable is perfectly correlated with itself."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(merged);"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot (or scatter plot matrix) is incredibly useful for a quick, high-level overview of relationships between multiple numerical variables simultaneously.\n",
        "\n",
        "1. Diagonal: Shows the distribution (histogram or KDE) of each individual variable.\n",
        "\n",
        "2. Off-Diagonal: Shows scatter plots for every combination of two variables, allowing you to visually identify correlations, clusters, or patterns."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the pair plot, you can gain a multi-faceted understanding:\n",
        "\n",
        "1. Individual Distributions (Diagonal): You'll see the shape of the distribution for Rating, Cost, Number_of_Reviews, and Followers. For example, Rating will likely be left-skewed (most ratings are high), while Cost, Number_of_Reviews, and Followers will probably be heavily right-skewed (many low values, few very high values).\n",
        "\n",
        "2. Pairwise Relationships (Off-Diagonal Scatter Plots):\n",
        "\n",
        "* Rating vs. Cost: You can visually assess if higher costs generally correspond to higher ratings, or if there's no clear pattern, or even if some high-cost restaurants have surprisingly low ratings.\n",
        "\n",
        "* Rating vs. Number_of_Reviews: See if restaurants with more reviews tend to have higher, lower, or more stable ratings.\n",
        "\n",
        "* Number_of_Reviews vs. Followers: This will likely show a positive correlation, indicating that as a reviewer writes more reviews, they tend to accumulate more followers. This reinforces findings from the specific scatter plot we did earlier.\n",
        "\n",
        "* Cost vs. Number_of_Reviews: Might reveal if more expensive restaurants are reviewed more or less frequently."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "1. Higher-cost restaurants tend to receive higher average ratings.\n",
        "\n",
        "2. Restaurants reviewed by individuals with a larger follower count are likely to achieve elevated ratings.\n",
        "\n",
        "3. Restaurants boasting a broader range of cuisines are associated with superior average ratings."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1 - Higher-cost restaurants tend to receive higher average ratings."
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This hypothesis compares the means of two independent groups (high-cost vs. low-cost restaurant ratings). A two-sample independent t-test is appropriate here.\n",
        "\n",
        "1. Null Hypothesis (H_0): There is no significant difference in the average ratings between high-cost and low-cost restaurants. (\n",
        "mu_high_cost=\n",
        "mu_low_cost)\n",
        "\n",
        "2. Alternative Hypothesis (H_1): High-cost restaurants have a significantly higher average rating than low-cost restaurants. (\n",
        "mu_high_cost\n",
        "mu_low_cost)\n",
        "\n",
        "Pre-computation: We'll first re-create the Cost_Range column if it's not already in the merged DataFrame."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import numpy as np # Ensure numpy is imported for numerical operations\n",
        "\n",
        "# Check null values in Cost column\n",
        "print(\"Nulls in 'Cost' column:\", merged['Cost'].isnull().sum())\n",
        "\n",
        "# Check unique values in Cost column\n",
        "print(\"Number of unique values in 'Cost' column:\", merged['Cost'].nunique())\n",
        "\n",
        "# Display a sample of unique values if there are few\n",
        "if merged['Cost'].nunique() < 20:\n",
        "    print(\"Unique values in 'Cost' column (if few):\", merged['Cost'].unique())\n",
        "\n",
        "# Create 'Cost_Range' bins, allowing duplicate bin edges to be dropped\n",
        "# This is crucial if many restaurants share the same 'Cost' value at quantile boundaries\n",
        "# Ensure 'Cost' column is numeric before quantiling\n",
        "if not pd.api.types.is_numeric_dtype(merged['Cost']):\n",
        "    merged['Cost'] = pd.to_numeric(merged['Cost'], errors='coerce').fillna(merged['Cost'].median()) # Re-coerce if needed\n",
        "\n",
        "# Handle potential issues with qcut if data is highly uniform\n",
        "try:\n",
        "    merged['Cost_Range'] = pd.qcut(merged['Cost'], q=3, labels=['Low Cost', 'Medium Cost', 'High Cost'], duplicates='drop')\n",
        "except ValueError as e:\n",
        "    print(f\"Warning: Could not create 3 distinct cost ranges due to data distribution. Error: {e}\")\n",
        "    # Fallback for highly uniform cost data: use simpler binning or fewer quantiles\n",
        "    merged['Cost_Range'] = pd.cut(merged['Cost'], bins=3, labels=['Low Cost', 'Medium Cost', 'High Cost'])\n",
        "\n",
        "\n",
        "# Extract ratings for high-cost and low-cost groups\n",
        "# Ensure to drop any NaNs in Rating that might exist\n",
        "high_cost_ratings = merged[merged['Cost_Range'] == 'High Cost']['Rating'].dropna()\n",
        "low_cost_ratings = merged[merged['Cost_Range'] == 'Low Cost']['Rating'].dropna()\n",
        "\n",
        "# Perform independent two-sample t-test\n",
        "# 'equal_var=False' is used for Welch's t-test, which is robust to unequal variances\n",
        "# 'alternative='greater'' tests if high_cost_ratings mean is greater than low_cost_ratings mean\n",
        "t_statistic, p_value = stats.ttest_ind(high_cost_ratings, low_cost_ratings, equal_var=False, alternative='greater')\n",
        "\n",
        "print(f\"\\nHypothesis 1: High-cost restaurants vs. Low-cost restaurants (Average Rating)\")\n",
        "print(f\"Mean Rating (High Cost): {high_cost_ratings.mean():.2f}\")\n",
        "print(f\"Mean Rating (Low Cost): {low_cost_ratings.mean():.2f}\")\n",
        "print(f\"T-statistic: {t_statistic:.2f}\")\n",
        "print(f\"P-value: {p_value:.3f}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(f\"Conclusion: Reject the Null Hypothesis. There is statistically significant evidence that high-cost restaurants have higher average ratings (p={p_value:.3f} < {alpha}).\")\n",
        "else:\n",
        "    print(f\"Conclusion: Fail to reject the Null Hypothesis. There is no statistically significant evidence that high-cost restaurants have higher average ratings (p={p_value:.3f} >= {alpha}).\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k6L-wtshWMSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We performed a Welch's two-sample independent t-test to obtain the P-value."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose Welch's t-test for the following reasons:\n",
        "\n",
        "1. Comparing Two Means: Our hypothesis involves comparing the average ratings (\n",
        "mu) of two distinct, independent groups (high-cost restaurants vs. low-cost restaurants).\n",
        "\n",
        "2. Independence: The ratings from one group of restaurants (e.g., high-cost) are independent of the ratings from the other group (low-cost).\n",
        "\n",
        "3. Robustness to Unequal Variances: Welch's t-test does not assume equal variances between the two groups, making it more robust and generally preferred over Student's t-test when group variances might differ, which is often the case with real-world data like restaurant ratings and costs.\n",
        "\n",
        "4. Directional Hypothesis: We had a directional alternative hypothesis (\n",
        "mu_high_cost\n",
        "mu_low_cost), so we used alternative='greater' in the test."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2 - Restaurants that are reviewed by reviewers with more followers will have a higher rating."
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This hypothesis explores the linear relationship between two continuous numerical variables: Reviewer_Followers (after log transformation due to its likely skewed distribution) and Rating. Pearson correlation coefficient is the appropriate measure, and we can test its significance.\n",
        "\n",
        "1. Null Hypothesis (H_0): There is no linear correlation between the number of reviewer followers and the rating they give. (\n",
        "rho=0)\n",
        "\n",
        "2. Alternative Hypothesis (H_1): There is a positive linear correlation between the number of reviewer followers and the rating they give. (\n",
        "rho0)"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "import numpy as np # For log1p\n",
        "\n",
        "# Log transform the Reviewer_Followers for better handling of skewed data\n",
        "# Ensure to drop any NaNs from the involved columns before correlation\n",
        "followers_log = np.log1p(merged['Reviewer_Followers'].dropna())\n",
        "ratings_for_corr = merged['Rating'].loc[followers_log.index].dropna() # Align ratings to non-NaN followers\n",
        "\n",
        "# Only proceed if there's enough data after dropping NaNs\n",
        "if len(followers_log) > 1 and len(ratings_for_corr) > 1:\n",
        "    correlation_coefficient, p_value = pearsonr(followers_log, ratings_for_corr)\n",
        "\n",
        "    print(f\"\\nHypothesis 2: Reviewer Followers vs. Rating Given\")\n",
        "    print(f\"Pearson Correlation (Log(1+Followers) vs. Rating): {correlation_coefficient:.2f}\")\n",
        "    print(f\"P-value: {p_value:.3f}\")\n",
        "\n",
        "    alpha = 0.05\n",
        "    # For a one-tailed test (positive correlation), we divide p-value by 2\n",
        "    # Note: pearsonr returns a two-tailed p-value by default.\n",
        "    p_value_one_tailed = p_value / 2\n",
        "\n",
        "    if correlation_coefficient > 0 and p_value_one_tailed < alpha:\n",
        "        print(f\"Conclusion: Reject the Null Hypothesis. There is statistically significant evidence of a positive linear correlation between reviewer followers and rating given (p={p_value_one_tailed:.3f} < {alpha}).\")\n",
        "    else:\n",
        "        print(f\"Conclusion: Fail to reject the Null Hypothesis. There is no statistically significant evidence of a positive linear correlation between reviewer followers and rating given (p={p_value_one_tailed:.3f} >= {alpha}).\")\n",
        "else:\n",
        "    print(\"\\nNot enough data to perform correlation analysis for Hypothesis 2 after dropping NaNs.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We performed a Pearson correlation test to obtain the P-value for the correlation coefficient.\n",
        "\n"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose Pearson correlation because:\n",
        "\n",
        "1. Relationship between Two Continuous Variables: It's used to quantify the strength and direction of a linear relationship between two continuous variables (Reviewer_Followers and Rating).\n",
        "\n",
        "2. Linearity Assumption: While we transform Reviewer_Followers with log1p to improve linearity and handle skewness, the underlying test checks for a linear relationship between the transformed variable and Rating.\n",
        "\n",
        "3. Parametric Test: It's a parametric test suitable for normally distributed data (or sufficiently large sample sizes where the Central Limit Theorem applies). The log transformation can also help in achieving a more normal distribution.\n",
        "\n",
        "4. Directional Hypothesis: We are specifically looking for a positive correlation."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3 - Restaurants that offer a wider variety of cuisines will have a higher rating."
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This hypothesis examines the relationship between the Number of Cuisines Offered (a numerical count) and the Average Rating of a restaurant. We'll use Pearson correlation to quantify this relationship.\n",
        "\n",
        "1. Null Hypothesis (H_0): There is no linear correlation between the number of cuisines a restaurant offers and its average rating. (\n",
        "rho=0)\n",
        "\n",
        "2. Alternative Hypothesis (H_1): There is a positive linear correlation between the number of cuisines a restaurant offers and its average rating. (\n",
        "rho0)"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "import numpy as np # For any potential numerical operations\n",
        "\n",
        "# Calculate the number of cuisines for each restaurant\n",
        "# First, ensure 'Cuisines' is treated as string and fill NaNs\n",
        "merged['Cuisines_Count'] = merged['Cuisines'].astype(str).apply(lambda x: len(x.split(', ')) if x != 'Not Available' else 0)\n",
        "\n",
        "# Aggregate average rating per restaurant and number of cuisines\n",
        "restaurant_cuisine_diversity = merged.groupby('Restaurant').agg(\n",
        "    Average_Rating=('Rating', 'mean'),\n",
        "    Num_Cuisines=('Cuisines_Count', 'first') # 'first' works because Cuisines_Count is constant per restaurant\n",
        ").reset_index()\n",
        "\n",
        "# Ensure to drop any NaNs from the involved columns before correlation\n",
        "num_cuisines_for_corr = restaurant_cuisine_diversity['Num_Cuisines'].dropna()\n",
        "avg_rating_for_corr = restaurant_cuisine_diversity['Average_Rating'].loc[num_cuisines_for_corr.index].dropna() # Align ratings to non-NaN cuisine counts\n",
        "\n",
        "# Only proceed if there's enough data after dropping NaNs\n",
        "if len(num_cuisines_for_corr) > 1 and len(avg_rating_for_corr) > 1:\n",
        "    correlation_coefficient, p_value = pearsonr(num_cuisines_for_corr, avg_rating_for_corr)\n",
        "\n",
        "    print(f\"\\nHypothesis 3: Number of Cuisines Offered vs. Average Restaurant Rating\")\n",
        "    print(f\"Pearson Correlation (Num Cuisines vs. Average Rating): {correlation_coefficient:.2f}\")\n",
        "    print(f\"P-value: {p_value:.3f}\")\n",
        "\n",
        "    alpha = 0.05\n",
        "    # For a one-tailed test (positive correlation), we divide p-value by 2\n",
        "    p_value_one_tailed = p_value / 2\n",
        "\n",
        "    if correlation_coefficient > 0 and p_value_one_tailed < alpha:\n",
        "        print(f\"Conclusion: Reject the Null Hypothesis. There is statistically significant evidence of a positive linear correlation between the number of cuisines offered and average restaurant rating (p={p_value_one_tailed:.3f} < {alpha}).\")\n",
        "    else:\n",
        "        print(f\"Conclusion: Fail to reject the Null Hypothesis. There is no statistically significant evidence of a positive linear correlation between the number of cuisines offered and average restaurant rating (p={p_value_one_tailed:.3f} >= {alpha}).\")\n",
        "else:\n",
        "    print(\"\\nNot enough data to perform correlation analysis for Hypothesis 3 after dropping NaNs.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We performed a Pearson correlation test to obtain the P-value for the correlation coefficient."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose Pearson correlation because:\n",
        "\n",
        "1. Relationship between Two Continuous Variables: It's used to quantify the strength and direction of a linear relationship between two continuous variables (Num_Cuisines and Average_Rating).\n",
        "\n",
        "2. Linearity Assumption: It assumes a linear relationship between the variables.\n",
        "\n",
        "3. Parametric Test: It's a parametric test suitable for sufficiently large sample sizes."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "#deleting duplicate value from review dataset\n",
        "review = review.drop_duplicates()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For handling missing values, I used a combination of imputation techniques tailored to the specific nature of each column and the type of data it contained. The primary goal was to ensure data quality and prevent errors or biases in subsequent analysis and modeling.\n",
        "\n",
        "Missing Value Imputation Techniques Used:\n",
        "1. Filling with 'Not Available' or Empty Strings (Categorical/Text)\n",
        "* Columns: hotel['Collections'], hotel['Timings'], review['Reviewer'], review['Review']\n",
        "\n",
        "* Technique: For categorical or text-based columns, missing values (NaNs) were replaced with a placeholder string like 'Not Available' or an empty string ('').\n",
        "\n",
        "* Reasoning:\n",
        "\n",
        "  * Preservation of Information: Instead of discarding rows, this approach preserves the existing data while explicitly marking the absence of a value.\n",
        "\n",
        "  * Meaningful Placeholder: For Collections and Timings, 'Not Available' is a clearer indicator that the information simply wasn't provided, rather than implying absence means a default value.\n",
        "\n",
        "  * Text Processing Compatibility: For Reviewer and Review, replacing NaNs with 'Anonymous' or empty strings prevents errors during text processing (like tokenization or NLP tasks) and doesn't distort distributions. An empty string for Review means the review text was simply blank.\n",
        "\n",
        "2. Filling with Median (Numerical)\n",
        "* Columns: hotel['Cost']\n",
        "\n",
        "* Technique: Missing values in the Cost column were imputed with the median value of the column.\n",
        "\n",
        "* Reasoning:\n",
        "\n",
        "  * Robustness to Outliers: The median is preferred over the mean for skewed numerical data (which cost often is) because it's less affected by extreme outliers. This prevents a few unusually high or low costs from significantly skewing the imputed value.\n",
        "\n",
        "  * Maintaining Distribution: Imputing with the median helps to preserve the overall distribution of the 'Cost' column more accurately than the mean would, especially in non-normal distributions.\n",
        "\n",
        "  * Edge Case Handling: A specific check was included to fill with 0 if the entire Cost column somehow became NaN after initial cleaning (though this is a rare edge case, it ensures the script doesn't crash).\n",
        "\n",
        "3. Filling with Zero (Numerical, Count-based)\n",
        "* Columns: review['Reviewer_Total_Review'] (renamed to Number_of_Reviews), review['Reviewer_Followers']\n",
        "\n",
        "* Technique: Missing values (NaNs resulting from pd.to_numeric(..., errors='coerce') for non-convertible entries) in these count-based numerical columns were filled with 0.\n",
        "\n",
        "* Reasoning:\n",
        "\n",
        "  * Logical Zero: For \"total reviews\" or \"followers,\" a missing or unparseable value logically implies that the reviewer has zero reviews or zero followers in the context of the available metadata. It's a natural default for absence in count data.\n",
        "\n",
        "  * Data Type Integrity: Filling with 0 (and then converting to int) maintains the integer data type for these count features.\n",
        "\n",
        "4. Dropping Rows (Critical Missing Data)\n",
        "* Columns: review['Rating'], review['Time']\n",
        "\n",
        "* Technique: Rows where the Rating or Time column had missing values (NaN/NaT) after initial conversion attempts were dropped.\n",
        "\n",
        "* Reasoning:\n",
        "\n",
        "  * Criticality: Rating is the primary target variable or a core analytical feature. A missing rating makes the review unusable for most analytical tasks.\n",
        "\n",
        "  * Irreparable Nature: Time is crucial for chronological analysis and feature extraction (like year, month, hour). If a timestamp cannot be parsed even after coercing errors, it indicates fundamentally bad or unrecoverable data. Trying to impute a date/time randomly or with a mean/median often doesn't make sense and can introduce significant bias.\n",
        "\n",
        "  * Minimal Loss (Assumed): This approach assumes that the number of rows dropped due to critical missing values is a small percentage of the overall dataset, making the trade-off acceptable for data integrity. If a large portion of data were missing in these critical columns, more complex imputation strategies might be considered, but often, the quality of derived insights would still be questionable."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#final check after dropping duplicates\n",
        "print(f\"Anymore duplicate left ? {review.duplicated().value_counts()}, unique values with {len(review[review.duplicated()])} duplication\")"
      ],
      "metadata": {
        "id": "lZlvlfH2ZVHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treating Missing Values - Restaurants"
      ],
      "metadata": {
        "id": "7injBR_7ZcFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "hotel.isnull().sum()"
      ],
      "metadata": {
        "id": "ozhMwtiiZhGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the null value in timing\n",
        "hotel[hotel['Timings'].isnull()]"
      ],
      "metadata": {
        "id": "Lkr7tTjvZlkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filling null value in timings column\n",
        "hotel.Timings.fillna(hotel.Timings.mode()[0], inplace = True)"
      ],
      "metadata": {
        "id": "R1Er0_a6ZpkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking null values in Collections\n",
        "missing_percentage = ((hotel['Collections'].isnull().sum())/(len(hotel['Collections'])))*100\n",
        "print(f'Percentage of missing value in Collections is {round(missing_percentage, 2)}%')"
      ],
      "metadata": {
        "id": "LJ58EhFOZuPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping collection column since it has more than 50% of null values\n",
        "hotel.drop('Collections', axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "ySsuD1UIZ0XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#final checking of missing value\n",
        "hotel.isnull().sum()"
      ],
      "metadata": {
        "id": "Y4jtjsk3Z5Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treating Missing Values - Reviews"
      ],
      "metadata": {
        "id": "CBjuWrv_aAm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#review missing value\n",
        "review.isnull().sum()"
      ],
      "metadata": {
        "id": "l3khcG4naCvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking null reviewer\n",
        "review[review['Reviewer'].isnull()]"
      ],
      "metadata": {
        "id": "VUOD1AiOaJw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking null Reviewer_Total_Review\n",
        "review[review['Reviewer_Total_Review'].isnull()]"
      ],
      "metadata": {
        "id": "4cBM2AQVaOz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping null values in reviewer and Reviewer_Total_Review column as all values are null for those column\n",
        "review = review.dropna(subset=['Reviewer','Reviewer_Total_Review'])"
      ],
      "metadata": {
        "id": "elH4yKs2aSfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#again checking the remaining values\n",
        "null_counts = [(x, a) for x, a in review.isnull().sum().items() if a > 0]\n",
        "\n",
        "# Print the columns with null values\n",
        "null_counts"
      ],
      "metadata": {
        "id": "Ge4EUGVLaVg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filling null values in review and reviewer follower column\n",
        "review = review.fillna({\"Review\": \"No Review\", \"Reviewer_Followers\": 0})"
      ],
      "metadata": {
        "id": "60_zdMfUaY-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final checking null values\n",
        "review.isnull().sum()"
      ],
      "metadata": {
        "id": "WbBdfgiqabxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merging both dataset\n",
        "merged = hotel.merge(review, on = 'Restaurant')\n",
        "merged.shape"
      ],
      "metadata": {
        "id": "Z9c0HkDUafS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "#Anamoly detection\n",
        "from sklearn.ensemble import IsolationForest\n",
        "#checking for normal distribution\n",
        "print(\"Skewness - Cost: %f\" % merged['Cost'].skew())\n",
        "print(\"Kurtosis - Cost: %f\" % merged['Cost'].kurt())\n",
        "print(\"Skewness - Reviewer_Followers: %f\" % merged['Reviewer_Followers'].skew())\n",
        "print(\"Kurtosis - Reviewer_Followers: %f\" % merged['Reviewer_Followers'].kurt())"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting graph for cost\n",
        "plt.scatter(range(merged.shape[0]), np.sort(merged['Cost'].values))\n",
        "plt.xlabel('index')\n",
        "plt.ylabel('Cost')\n",
        "plt.title(\"Cost distribution\")\n",
        "sns.despine()"
      ],
      "metadata": {
        "id": "FElrOU5ybXxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution of cost\n",
        "sns.distplot(merged['Cost'])\n",
        "plt.title(\"Distribution of Cost\")\n",
        "sns.despine()"
      ],
      "metadata": {
        "id": "yeDSiEjSbbtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot for reviewer follower\n",
        "plt.scatter(range(merged.shape[0]), np.sort(merged['Reviewer_Followers'].values))\n",
        "plt.xlabel('index')\n",
        "plt.ylabel('Reviewer_Followers')\n",
        "plt.title(\"Reviewer_Followers distribution\")\n",
        "sns.despine()"
      ],
      "metadata": {
        "id": "l-4jO4KTbfbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution of Reviewer_Followers\n",
        "sns.distplot(merged['Reviewer_Followers'])\n",
        "plt.title(\"Distribution of Reviewer_Followers\")\n",
        "sns.despine()"
      ],
      "metadata": {
        "id": "-SJaK3BIbjOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#isolation forest for anamoly detection on cost\n",
        "isolation_forest = IsolationForest(n_estimators=100, contamination=0.01)\n",
        "isolation_forest.fit(merged['Cost'].values.reshape(-1, 1))\n",
        "merged['anomaly_score_univariate_Cost'] = isolation_forest.decision_function(merged['Cost'].values.reshape(-1, 1))\n",
        "merged['outlier_univariate_Cost'] = isolation_forest.predict(merged['Cost'].values.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "zDPEKSQibneM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chart to visualize outliers\n",
        "xx = np.linspace(merged['Cost'].min(), merged['Cost'].max(), len(merged)).reshape(-1,1)\n",
        "anomaly_score = isolation_forest.decision_function(xx)\n",
        "outlier = isolation_forest.predict(xx)\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(xx, anomaly_score, label='anomaly score')\n",
        "plt.fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score),\n",
        "where=outlier==-1, color='r',\n",
        "alpha=.4, label='outlier region')\n",
        "plt.legend()\n",
        "plt.ylabel('anomaly score')\n",
        "plt.xlabel('Cost')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "ZxBlAShnbrX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#isolation forest for anamoly detection of reviewer follower\n",
        "isolation_forest = IsolationForest(n_estimators=100, contamination=0.01)\n",
        "isolation_forest.fit(merged['Reviewer_Followers'].values.reshape(-1, 1))\n",
        "merged['anomaly_score_univariate_follower'] = isolation_forest.decision_function(\n",
        "    merged['Reviewer_Followers'].values.reshape(-1, 1))\n",
        "merged['outlier_univariate_follower'] = isolation_forest.predict(\n",
        "    merged['Reviewer_Followers'].values.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "9bk6iVNVbu2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chat to visualize outliers in reviwer follower column\n",
        "xx = np.linspace(merged['Reviewer_Followers'].min(), merged['Reviewer_Followers'].max(), len(merged)).reshape(-1,1)\n",
        "anomaly_score = isolation_forest.decision_function(xx)\n",
        "outlier = isolation_forest.predict(xx)\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(xx, anomaly_score, label='anomaly score')\n",
        "plt.fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score),\n",
        "where=outlier==-1, color='r',\n",
        "alpha=.4, label='outlier region')\n",
        "plt.legend()\n",
        "plt.ylabel('anomaly score')\n",
        "plt.xlabel('Reviewer_Followers')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "o2MiggVybyS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treating Outliers"
      ],
      "metadata": {
        "id": "SdUiZeqZb3Av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np # Often useful for numerical operations\n",
        "\n",
        "# Ensure 'merged' DataFrame is defined from previous steps\n",
        "# (Assuming merged is already loaded and preprocessed)\n",
        "\n",
        "# Handling Outliers & Outlier treatments\n",
        "# To separate the symmetric distributed features and skew symmetric distributed features\n",
        "\n",
        "symmetric_feature = []\n",
        "non_symmetric_feature = []\n",
        "\n",
        "# Iterate only over numerical columns\n",
        "for i in merged.select_dtypes(include=np.number).columns: # Use select_dtypes to get only numerical columns\n",
        "    # Ensure there are no NaNs in the column before calculating mean/median,\n",
        "    # as NaNs can sometimes cause issues or skew results unexpectedly.\n",
        "    # Alternatively, ensure your preprocessing has handled all NaNs.\n",
        "    if merged[i].isnull().any():\n",
        "        print(f\"Warning: Column '{i}' contains NaN values. Mean/Median calculation might be affected.\")\n",
        "        # You might choose to skip this column or fill NaNs before calculation here\n",
        "        # For simplicity, we'll proceed, but be aware of NaNs' impact.\n",
        "\n",
        "    # Calculate mean and median only for numeric columns\n",
        "    column_mean = merged[i].mean()\n",
        "    column_median = merged[i].median()\n",
        "\n",
        "    # Perform the comparison\n",
        "    if abs(column_mean - column_median) < 0.2:\n",
        "        symmetric_feature.append(i)\n",
        "    else:\n",
        "        non_symmetric_feature.append(i)\n",
        "\n",
        "# Getting Symmetric Distributed Features\n",
        "print(\"Symmetric Distributed Features : -\", symmetric_feature)\n",
        "# Getting Skew Symmetric Distributed Features\n",
        "print(\"Skew Symmetric Distributed Features : -\", non_symmetric_feature)"
      ],
      "metadata": {
        "id": "iNcA6vA9b5UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Skew Symmetric features defining upper and lower boundry\n",
        "#Outer Fence\n",
        "def outlier_treatment_skew(df,feature):\n",
        "  IQR= df[feature].quantile(0.75)- df[feature].quantile(0.25)\n",
        "  lower_bridge =df[feature].quantile(0.25)- 1.5*IQR\n",
        "  upper_bridge =df[feature].quantile(0.75)+ 1.5*IQR\n",
        "  # print(f'upper : {upper_bridge} lower : {lower_bridge}')\n",
        "  return upper_bridge,lower_bridge"
      ],
      "metadata": {
        "id": "_e5S7KENcK76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restricting the data to lower and upper boundary for cost in hotel dataset\n",
        "#lower limit capping\n",
        "hotel.loc[hotel['Cost']<= outlier_treatment_skew(df=hotel,\n",
        "  feature='Cost')[1], 'Cost']=outlier_treatment_skew(df=hotel,feature='Cost')[1]\n",
        "\n",
        "#upper limit capping\n",
        "hotel.loc[hotel['Cost']>= outlier_treatment_skew(df=hotel,\n",
        "  feature='Cost')[0], 'Cost']=outlier_treatment_skew(df=hotel,feature='Cost')[0]"
      ],
      "metadata": {
        "id": "5z-N8UIWcOO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restricting the data to lower and upper boundary for Reviewer followers in review dataset\n",
        "#lower limit capping\n",
        "review.loc[review['Reviewer_Followers']<= outlier_treatment_skew(df=review,\n",
        "  feature='Reviewer_Followers')[1], 'Reviewer_Followers']=outlier_treatment_skew(\n",
        "      df=review,feature='Reviewer_Followers')[1]\n",
        "\n",
        "#upper limit capping\n",
        "review.loc[review['Reviewer_Followers']>= outlier_treatment_skew(df=review,\n",
        "  feature='Reviewer_Followers')[0], 'Reviewer_Followers']=outlier_treatment_skew(\n",
        "      df=review,feature='Reviewer_Followers')[0]"
      ],
      "metadata": {
        "id": "cKX0EWyjcSYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping the columns created while outliers treatment\n",
        "merged.drop(columns =['anomaly_score_univariate_Cost','outlier_univariate_Cost',\n",
        "  'anomaly_score_univariate_follower','outlier_univariate_follower'], inplace = True)"
      ],
      "metadata": {
        "id": "CugUEexjcVSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used the Isolation Forest algorithm for outlier detection and a capping (or winsorization) method based on the Interquartile Range (IQR) for outlier treatment.\n",
        "\n",
        "Techniques used and the reasoning:\n",
        "\n",
        "1. Outlier Detection Technique: Isolation Forest\n",
        "* Columns Applied To: Cost and Reviewer_Followers\n",
        "\n",
        "* Technique: Isolation Forest is an unsupervised machine learning algorithm used for anomaly detection. It works by \"isolating\" anomalies (outliers) in a dataset. It builds a forest of random trees, and the anomalies are points that are separated from the rest of the data points with shorter paths in the trees.\n",
        "\n",
        "  * n_estimators=100: The number of trees in the forest. More trees generally lead to more robust results.\n",
        "\n",
        "  * contamination=0.01: This parameter estimates the proportion of outliers in the dataset. Setting it to 0.01 means the model expects about 1% of the data to be outliers.\n",
        "\n",
        "  * decision_function(): Returns the anomaly score for each data point. Lower scores indicate a higher likelihood of being an outlier.\n",
        "\n",
        "  * predict(): Assigns a label of -1 for outliers and 1 for inliers.\n",
        "\n",
        "* Why it was used:\n",
        "\n",
        "  * Effectiveness for High-Dimensional Data (though used univariately here): While robust in high dimensions, Isolation Forest can also effectively identify outliers in univariate distributions, especially those that are highly skewed (like Cost and Reviewer_Followers as indicated by their high skewness and kurtosis values).\n",
        "\n",
        "  * No Assumption of Data Distribution: Unlike statistical methods that assume a Gaussian distribution, Isolation Forest makes no such assumptions, making it suitable for skewed data.\n",
        "\n",
        "  * Efficiency: It's generally efficient for large datasets.\n",
        "\n",
        "  * Visualization: The anomaly scores and outlier regions can be clearly visualized, as shown in your plots, providing a clear boundary for what the model considers anomalous.\n",
        "\n",
        "2. Outlier Treatment Technique: IQR-based Capping (Winsorization)\n",
        "Columns Applied To: Cost (in the hotel DataFrame) and Reviewer_Followers (in the review DataFrame)\n",
        "\n",
        "* Technique: For features identified as skew-symmetric (meaning their distribution is skewed, e.g., 'Cost', 'Reviewer_Followers'), an IQR-based method was used to define upper and lower boundaries. Values falling outside these boundaries were capped (or winsorized) at the boundary values.\n",
        "\n",
        "  * outlier_treatment_skew(df, feature) function: This function calculates the upper and lower fences:\n",
        "\n",
        "  * IQR = Q3 - Q1 (where Q3 is the 75th percentile and Q1 is the 25th percentile).\n",
        "\n",
        "  * Lower_Bridge = Q1 - 1.5 * IQR\n",
        "\n",
        "  * Upper_Bridge = Q3 + 1.5 * IQR\n",
        "\n",
        "* Capping Implementation:\n",
        "\n",
        "  * df.loc[df[feature] <= lower_bridge, feature] = lower_bridge: Any value below the lower fence is replaced with the lower fence itself.\n",
        "\n",
        "  * df.loc[df[feature] >= upper_bridge, feature] = upper_bridge: Any value above the upper fence is replaced with the upper fence itself.\n",
        "\n",
        "* Why it was used:\n",
        "\n",
        "  * Suitability for Skewed Data: The IQR method is robust to skewness in data, unlike methods that rely on standard deviation (which assume a normal distribution). The skewness values (1.15 for Cost, 10.09 for Reviewer_Followers) strongly suggest that these features are not normally distributed, making IQR-based treatment appropriate.\n",
        "\n",
        "  * Mitigation of Extreme Values: Capping reduces the influence of extreme outliers without completely removing the data points, thus preserving some information. This is particularly useful for features like Cost or Reviewer_Followers where very high values might be legitimate but disproportionately affect statistical models.\n",
        "\n",
        "  * Maintaining Data Size: Unlike dropping rows, capping keeps all original data points, preventing a reduction in dataset size.\n",
        "\n",
        "  * Practicality: It's a widely accepted and relatively simple method for outlier treatment in skewed distributions.\n",
        "\n",
        "Note on Column Dropping:\n",
        "\n",
        "Additionally, the columns created during the Isolation Forest detection (anomaly_score_univariate_Cost, outlier_univariate_Cost, etc.) were dropped from the merged DataFrame after the treatment. This is a cleanup step to remove temporary columns that were only needed for outlier detection and visualization, not for the final analysis or modeling."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Encode your categorical columns\n",
        "\n",
        "#categorial encoding using pd.getdummies\n",
        "#new df with important categories\n",
        "cluster_dummy = hotel[['Restaurant','Cuisines']]\n",
        "#spliting cuisines as they are separted with comma and converting into list\n",
        "cluster_dummy['Cuisines'] = cluster_dummy['Cuisines'].str.split(',')\n",
        "#using explode converting list to unique individual items\n",
        "cluster_dummy = cluster_dummy.explode('Cuisines')\n",
        "#removing extra trailing space from cuisines after exploded\n",
        "cluster_dummy['Cuisines'] = cluster_dummy['Cuisines'].apply(lambda x: x.strip())\n",
        "#using get dummies to get dummies for cuisines\n",
        "cluster_dummy = pd.get_dummies(cluster_dummy, columns=[\"Cuisines\"], prefix=[\"Cuisines\"])\n",
        "\n",
        "#checking if the values are correct\n",
        "# cluster_dummy.loc[:, cluster_dummy.columns.str.startswith('Cuisines_')].eq(1)[:5].T\n",
        "cluster_dummy.loc[:, cluster_dummy.columns.str.startswith('Cuisines_')].idxmax(1)[:6]\n",
        "\n",
        "#replacing cuisines_ from columns name - for better understanding run seperatly\n",
        "\n",
        "# cluster_dummy.columns = cluster_dummy.columns.str.replace(\" \",\"\")\n",
        "cluster_dummy.columns = cluster_dummy.columns.str.replace(\"Cuisines_\",\"\")\n",
        "# cluster_dummy = cluster_dummy.groupby(cluster_dummy.columns, axis=1).sum()\n",
        "\n",
        "#grouping each restaurant as explode created unnecessary rows\n",
        "cluster_dummy = cluster_dummy.groupby(\"Restaurant\").sum().reset_index()"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#total cuisine count\n",
        "hotel['Total_Cuisine_Count'] = hotel['Cuisines'].apply(lambda x : len(x.split(',')))"
      ],
      "metadata": {
        "id": "ydslBeaMd71L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding average rating - will remove 5 unrated restaurant from 105 restaurant\n",
        "avg_hotel_rating.rename(columns = {'Rating':'Average_Rating'}, inplace =True)\n",
        "hotel = hotel.merge(avg_hotel_rating[['Average_Rating','Restaurant']], on = 'Restaurant')\n",
        "hotel.head(1)"
      ],
      "metadata": {
        "id": "4dlo4-Hod-6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding cost column to the new dataset\n",
        "cluster_dummy = hotel[['Restaurant','Cost','Average_Rating','Total_Cuisine_Count'\n",
        "                      ]].merge(cluster_dummy, on = 'Restaurant')"
      ],
      "metadata": {
        "id": "lhvpzqC-eDKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_dummy.shape"
      ],
      "metadata": {
        "id": "0pGnvT_5eG1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternate Method for Creating Dummies"
      ],
      "metadata": {
        "id": "ws0eUxlleQ7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating data frame for categorial encoding\n",
        "cluster_df = hotel[['Restaurant','Cuisines','Cost','Average_Rating','Total_Cuisine_Count']]"
      ],
      "metadata": {
        "id": "SO6bNrPneTOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new dataframe for clustering\n",
        "cluster_df = pd.concat([cluster_df,pd.DataFrame(columns=list(cuisine_dict.keys()))])"
      ],
      "metadata": {
        "id": "HKyrKq2YeXDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating categorial feature for cuisine\n",
        "#iterate over every row in the dataframe\n",
        "for i, row in cluster_df.iterrows():\n",
        "  # iterate over the new columns\n",
        "  for column in list(cluster_df.columns):\n",
        "      if column not in ['Restaurant','Cost','Cuisines','Average_Rating','Total_Cuisine_Count']:\n",
        "        # checking if the column is in the list of cuisines available for that row\n",
        "        if column in row['Cuisines']:\n",
        "          #assign it as 1 else 0\n",
        "          cluster_df.loc[i,column] = 1\n",
        "        else:\n",
        "          cluster_df.loc[i,column] = 0"
      ],
      "metadata": {
        "id": "C-6qPtmpebXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#result from encoding\n",
        "cluster_df.head(2).T"
      ],
      "metadata": {
        "id": "kleBXVv6eexd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used One-Hot Encoding (via pd.get_dummies) for the 'Cuisines' column, which is a common and effective technique for handling categorical data, especially when there are no inherent ordinal relationships between categories.\n",
        "\n",
        "Techniques used and the reasoning:\n",
        "1. One-Hot Encoding (for 'Cuisines')\n",
        "* Technique: You transformed the Cuisines column (which contains multiple, comma-separated cuisines per restaurant) into a series of binary (0 or 1) columns. Each new column represents a unique cuisine, and a 1 indicates that the restaurant offers that particular cuisine, while a 0 indicates it does not.\n",
        "\n",
        "  * Steps involved:\n",
        "\n",
        "    1. cluster_dummy['Cuisines'] = cluster_dummy['Cuisines'].str.split(','): Split the comma-separated string into a list of cuisines for each restaurant.\n",
        "\n",
        "    2.  cluster_dummy = cluster_dummy.explode('Cuisines'): Transformed the DataFrame such that each cuisine from the list gets its own row, duplicating the restaurant information.\n",
        "\n",
        "    3. cluster_dummy['Cuisines'] = cluster_dummy['Cuisines'].apply(lambda x: x.strip()): Cleaned up any leading/trailing whitespace from the cuisine names.\n",
        "\n",
        "    4. cluster_dummy = pd.get_dummies(cluster_dummy, columns=[\"Cuisines\"], prefix=[\"Cuisines\"]): Applied the actual one-hot encoding, creating binary columns for each unique cuisine.\n",
        "\n",
        "    5. cluster_dummy.columns = cluster_dummy.columns.str.replace(\"Cuisines_\",\"\"): Renamed the columns for clarity (e.g., Cuisines_Chinese becomes Chinese).\n",
        "\n",
        "    6. cluster_dummy = cluster_dummy.groupby(\"Restaurant\").sum().reset_index(): Aggregated the dummy variables back to the restaurant level. Since a restaurant offering a cuisine would have a 1 in the exploded rows, summing them up effectively gives a 1 if the restaurant offers that cuisine at least once, and 0 otherwise.\n",
        "\n",
        "* Why it was used:\n",
        "\n",
        "  * Nominal Categorical Data: Cuisines are nominal categories (there's no inherent order or ranking between \"Chinese\" and \"Italian\"). One-hot encoding is ideal for such data because it avoids implying any false ordinal relationships that numerical encoding might introduce.\n",
        "\n",
        "  * Multi-Label Categories: Restaurants can offer multiple cuisines. The explode and then get_dummies approach correctly handles this multi-label scenario, where a single restaurant can have 1s across several cuisine columns.\n",
        "\n",
        "  * Compatibility with Machine Learning Models: Most machine learning algorithms cannot directly work with text categories. One-hot encoding converts these categories into a numerical format that models can understand and process.\n",
        "\n",
        "  * No Information Loss: This method preserves all the information about which cuisines a restaurant offers.\n",
        "\n",
        "2. Implicit Numerical Encoding/Feature Creation (for Total_Cuisine_Count)\n",
        "Column: hotel['Total_Cuisine_Count']\n",
        "\n",
        "* Technique: You created a new numerical feature by counting the number of cuisines each restaurant offers. This is not a direct categorical encoding of an existing column, but rather a feature derived from a categorical one.\n",
        "\n",
        "  * Step involved: hotel['Total_Cuisine_Count'] = hotel['Cuisines'].apply(lambda x : len(x.split(',')))\n",
        "\n",
        "* Why it was used:\n",
        "\n",
        "  * Capturing Diversity: This numerical feature directly quantifies the diversity of cuisines offered by a restaurant, which can be an important predictor or descriptive statistic.\n",
        "\n",
        "  * Simplicity & Interpretability: It's a straightforward and easily interpretable metric.\n",
        "\n",
        "  * Complementary to One-Hot Encoding: While one-hot encoding tells you which specific cuisines are offered, Total_Cuisine_Count tells you how many distinct cuisines are offered. Both can be valuable for different analytical purposes.\n",
        "\n",
        "Alternate Method (Manual One-Hot-like Encoding)\n",
        "Presented an \"Alternate Method\" using a loop to manually assign 1s and 0s based on a predefined cuisine_dict (which wasn't shown in the provided snippet but is implied).\n",
        "\n",
        "* Technique: This involves iterating through rows and columns to manually set binary flags based on whether a cuisine is present in the Cuisines string.\n",
        "\n",
        "* Why it was included: While pd.get_dummies is generally more efficient and recommended for this task, a manual loop can sometimes be used to achieve a similar one-hot encoding effect, especially when there's complex logic or a need to map to pre-existing specific cuisine columns. However, for a straightforward multi-label scenario like this, pd.get_dummies with explode is typically preferred for its efficiency and conciseness."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "#creating new df for text processing of sentiment analysis\n",
        "sentiment_df = review[['Reviewer','Restaurant','Rating','Review']]\n",
        "#analysing two random sample\n",
        "sentiment_df.sample(2)"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting index\n",
        "sentiment_df = sentiment_df.reset_index()\n",
        "sentiment_df['index'] = sentiment_df.index"
      ],
      "metadata": {
        "id": "NKWysV7n5F4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.sample(2)"
      ],
      "metadata": {
        "id": "0y01lva65JzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "import contractions\n",
        "# applying fuction for contracting text\n",
        "sentiment_df['Review']=sentiment_df['Review'].apply(lambda x:contractions.fix(x))"
      ],
      "metadata": {
        "id": "Xy69RQio5OF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "sentiment_df['Review'] = sentiment_df['Review'].str.lower()"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "def remove_punctuation(text):\n",
        "  '''a function for removing punctuation'''\n",
        "\n",
        "  # replacing the punctuations with no space,\n",
        "  # which in effect deletes the punctuation marks\n",
        "  translator = str.maketrans('', '', string.punctuation)\n",
        "  # return the text stripped of punctuation marks\n",
        "  return text.translate(translator)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove punctuation using function created\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(remove_punctuation)\n",
        "sentiment_df.sample(5)"
      ],
      "metadata": {
        "id": "ccJEyOny5a4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "\n",
        "# Remove links\n",
        "sentiment_df[\"Review\"] = sentiment_df[\"Review\"].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
        "\n",
        "# Remove digits\n",
        "sentiment_df[\"Review\"] = sentiment_df[\"Review\"].apply(lambda x: re.sub(r\"\\d+\", \"\", x))"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to extract location of the restaurant\n",
        "def get_location(link):\n",
        "  link_elements = link.split(\"/\")\n",
        "  return link_elements[3]\n",
        "\n",
        "#create a location feature\n",
        "hotel['Location'] = hotel['Links'].apply(get_location)\n",
        "hotel.sample(2)"
      ],
      "metadata": {
        "id": "g704EB1d5knX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "# extracting the stopwords from nltk library\n",
        "sw = stopwords.words('english')"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to remove stopwords\n",
        "def delete_stopwords(text):\n",
        "  '''a function for removing the stopword'''\n",
        "  # removing the stop words and lowercasing the selected words\n",
        "  text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
        "  # joining the list of words with space separator\n",
        "  return \" \".join(text)"
      ],
      "metadata": {
        "id": "UleQABq65sl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calling function to remove stopwords\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(delete_stopwords)"
      ],
      "metadata": {
        "id": "nHn4Y2PH5v0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "sentiment_df['Review'] =sentiment_df['Review'].apply(lambda x: \" \".join(x.split()))"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#random sample\n",
        "sentiment_df.sample(2)"
      ],
      "metadata": {
        "id": "RfDd8llI541u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Rephrase Text"
      ],
      "metadata": {
        "id": "weujFCsl6ZZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.sample(2)"
      ],
      "metadata": {
        "id": "KT7LI93w6fLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#applying Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Create a lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Lemmatize the 'Review' column\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(lemmatize_tokens)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.sample(2)"
      ],
      "metadata": {
        "id": "sytmON0T8A7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Lemmatization:*\n",
        "\n",
        "I've implemented lemmatization using nltk.stem.WordNetLemmatizer. This process reduces words to their base or dictionary form (lemma). For example, words like \"running,\" \"ran,\" and \"runs\" are all converted to \"run.\"\n",
        "\n",
        "*Why Lemmatization Was Chosen:*\n",
        "\n",
        "1. Semantic Accuracy: Lemmatization aims to return a valid word from a dictionary, considering the word's part of speech and context. This results in more meaningful base forms compared to stemming, which often just truncates words and might produce non-dictionary terms (e.g., \"automat\" from \"automatic\").\n",
        "\n",
        "21. Reduced Vocabulary: By unifying different inflected forms of a word, lemmatization helps reduce the overall size of your vocabulary. A smaller, more consistent vocabulary can improve the performance and efficiency of text-based machine learning models by reducing sparsity.\n",
        "\n",
        "3. Enhanced Feature Representation: When analyzing text, you want words with the same core meaning to be treated similarly. Lemmatization ensures that related words are grouped, providing a cleaner and more accurate representation of the text's content, which is beneficial for tasks like sentiment analysis or text clustering."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
        "vectorizer.fit(sentiment_df['Review'].values)\n",
        "#creating independent variable for sentiment analysis\n",
        "X_tfidf = vectorizer.transform(sentiment_df['Review'].values)"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bag of Words\n",
        "tokenized_text = []\n",
        "for token in sentiment_df['Review']:\n",
        "    tokenized_text.append(token)\n",
        "\n",
        "#creating token dict\n",
        "tokens_dict = gensim.corpora.Dictionary(tokenized_text)\n",
        "\n",
        "#print token dict\n",
        "#tokens_dict.token2id"
      ],
      "metadata": {
        "id": "0JM133789dLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using tokens_dict.doc2bow() to generate BoW features for each tokenized course\n",
        "texts_bow = [tokens_dict.doc2bow(text) for text in tokenized_text]\n",
        "\n",
        "#creating a new text_bow dataframe based on the extracted BoW features\n",
        "tokens = []\n",
        "bow_values = []\n",
        "doc_indices = []\n",
        "doc_ids = []\n",
        "for text_idx, text_bow in enumerate(texts_bow):\n",
        "    for token_index, token_bow in text_bow:\n",
        "        token = tokens_dict.get(token_index)\n",
        "        tokens.append(token)\n",
        "        bow_values.append(token_bow)\n",
        "        doc_indices.append(text_idx)\n",
        "        doc_ids.append(sentiment_df[\"Restaurant\"][text_idx])\n",
        "\n",
        "bow_dict = {\"doc_index\": doc_indices,\n",
        "            \"doc_id\": doc_ids,\n",
        "            \"token\": tokens,\n",
        "            \"bow\": bow_values,\n",
        "            }\n",
        "bows_df = pd.DataFrame(bow_dict)\n",
        "bows_df.head()"
      ],
      "metadata": {
        "id": "V2i131fe9hZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Vectorization Techniques Used\n",
        "\n",
        "1. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "I used TfidfVectorizer from sklearn.feature_extraction.text to transform the Review column.\n",
        "\n",
        "* How it works: TF-IDF assigns a numerical weight to each word in a document, reflecting how important a word is to that document within a collection of documents (corpus).\n",
        "\n",
        "  * Term Frequency (TF): How often a word appears in a specific document.\n",
        "\n",
        "  * Inverse Document Frequency (IDF): A measure of how rare or common a word is across all documents in the corpus. Words that are common across many documents (like \"the\" or \"a\") get a lower IDF score, while unique or specific words get a higher IDF score.\n",
        "\n",
        "  * TF-IDF is the product of TF and IDF.\n",
        "\n",
        "* Why it was used:\n",
        "\n",
        "  *  Captures Importance: TF-IDF is excellent for capturing the importance of words not just by their frequency in a single review, but also by their rarity across all reviews. This means common, less informative words (like stopwords, even after removal) are down-weighted, while more distinctive, \"key\" words are given higher scores.\n",
        "\n",
        "  * Handles Stopwords Implicitly: While you manually removed stopwords, TF-IDF naturally assigns low weights to very common words, making it robust to variations in preprocessing.\n",
        "\n",
        "  * Numerical Representation: It provides a numerical representation of text, which is a required input for most machine learning models.\n",
        "\n",
        "  * Feature Scaling: The values produced by TF-IDF are typically normalized, aiding in better performance for many models.\n",
        "\n",
        "2. Bag of Words (BoW)\n",
        "I implemented a Bag of Words model using gensim.corpora.Dictionary and doc2bow.\n",
        "\n",
        "* How it works: BoW represents text as an unordered collection of words, disregarding grammar and word order, but keeping track of the frequency of each word.\n",
        "\n",
        "  * Create a Dictionary (tokens_dict): This maps every unique word in your entire corpus to a unique integer ID.\n",
        "\n",
        "  * Convert to Bag-of-Words Format (texts_bow): For each review, it creates a list of (word_id, frequency) pairs, indicating how many times each word (by its ID) appears in that specific review.\n",
        "\n",
        "  * DataFrame Creation (bows_df): You then converted this list of lists into a DataFrame, showing which token appeared how many times in which document.\n",
        "\n",
        "* Why it was used:\n",
        "\n",
        "  * Simplicity and Interpretability: BoW is a straightforward and intuitive way to convert text into a numerical format. It's easy to understand what each numerical feature represents (the count of a specific word).\n",
        "\n",
        "  * Foundation for NLP: It's a fundamental technique in NLP and serves as the basis for many other more complex text representation models.\n",
        "\n",
        "  * Feature Creation: It creates a numerical vector (a \"bag\" of word counts) for each document, which can then be fed into machine learning algorithms.\n",
        "\n",
        "Both TF-IDF and BoW are essential techniques for converting unstructured text into structured numerical data that machine learning models can process. TF-IDF often provides a more nuanced representation of word importance than simple word counts (BoW)."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restaurant"
      ],
      "metadata": {
        "id": "Y4i87yBY-nAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "hotel.shape"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#columns for dataset\n",
        "hotel.columns"
      ],
      "metadata": {
        "id": "cSuLBnNz-yPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping columns\n",
        "hotel = hotel.drop(columns = ['Links','Location'], axis = 1)"
      ],
      "metadata": {
        "id": "rEGk8r8K-7Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hotel.shape"
      ],
      "metadata": {
        "id": "ifL_A0XN--GE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new dataframe to be used for clustering i.e dropping the unimportant column\n",
        "cluster_df.shape"
      ],
      "metadata": {
        "id": "j7CnzS95_BZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping cuisine and restaurant from cluster_df\n",
        "cluster_df = cluster_df.drop(columns = ['Restaurant','Cuisines'], axis = 1)"
      ],
      "metadata": {
        "id": "gOTtCJ9b_KHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_df.sample(1)"
      ],
      "metadata": {
        "id": "b3npHtHX_NOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#alternatively using other variable created earlier during categorial creation\n",
        "cluster_dummy.shape"
      ],
      "metadata": {
        "id": "rbqTQlkT_Q9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#review data shape\n",
        "review.shape"
      ],
      "metadata": {
        "id": "r8KQ5P03_UYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#review column\n",
        "review.columns"
      ],
      "metadata": {
        "id": "gXcCtA4h_XV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new binary feature called sentiment based on rating which has 1 = positive and 0 = negative\n",
        "sentiment_df['Sentiment'] = sentiment_df['Rating'].apply(\n",
        "    lambda x: 1 if x >=sentiment_df['Rating'].mean() else 0)  #1 = positive # 0 = negative"
      ],
      "metadata": {
        "id": "TkMItxFt_bVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sentiment data frame\n",
        "sentiment_df.sample(2)"
      ],
      "metadata": {
        "id": "a3NegQmz_e8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "hotel.columns"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature selcted for clustering\n",
        "cluster_df.columns"
      ],
      "metadata": {
        "id": "ivaPyqvu_nMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_dummy.columns"
      ],
      "metadata": {
        "id": "2gcSfbjW_qcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review.columns"
      ],
      "metadata": {
        "id": "O4HxbLO6_tEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature selected for sentiment analysis\n",
        "sentiment_df.columns"
      ],
      "metadata": {
        "id": "tqYN3raB_wPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I didn't use formal feature selection methods. Instead, I relied on manual selection based on my understanding of the data and what would be relevant for the specific tasks, like clustering and sentiment analysis.\n",
        "\n",
        "*My Feature Selection Approach*\n",
        "\n",
        "For both the clustering and sentiment analysis tasks, I directly chose the columns I believed were most relevant, rather than employing automated feature selection algorithms. This approach is often driven by domain knowledge and the immediate goals of the analysis.\n",
        "\n",
        "For Restaurant Clustering\n",
        "\n",
        "For clustering restaurants, I selected:\n",
        "\n",
        "* Cost: I figured the price range would be a major factor in grouping similar restaurants.\n",
        "\n",
        "* Average_Rating: Naturally, a restaurant's overall rating is a critical indicator of its quality and popularity, so it had to be in there.\n",
        "\n",
        "* Total_Cuisine_Count: I engineered this feature to quantify how diverse a restaurant's menu is. I thought this would help distinguish between specialized eateries and those offering a broad range of dishes.\n",
        "\n",
        "* Individual Cuisine Dummy Variables: These are the one-hot encoded columns for each specific cuisine (like 'Chinese', 'Italian', etc.). I knew these were essential to group restaurants based on the types of food they serve.\n",
        "\n",
        "My rationale here was to pick features that directly describe a restaurant's core identity, offerings, and perceived quality. These attributes felt like they'd naturally form distinct clusters of restaurants.\n",
        "\n",
        "For Sentiment Analysis\n",
        "\n",
        "For preparing the data for sentiment analysis, I initially picked:\n",
        "\n",
        "* Reviewer: To keep track of who wrote the review.\n",
        "\n",
        "* Restaurant: To know which restaurant the review was about.\n",
        "\n",
        "* Rating: This is super important because it often serves as the numeric representation of the sentiment (e.g., a 5-star rating usually means positive sentiment).\n",
        "\n",
        "* Review: This is the actual text I needed to analyze for sentiment.\n",
        "\n",
        "* index: Just a simple identifier I added.\n",
        "\n",
        "Later, I anticipated a Sentiment column, which would be my target variable, either directly derived from Rating or as the output of a sentiment model.\n",
        "\n",
        "My goal here was to gather all the raw information necessary for breaking down and analyzing the sentiment expressed in the reviews.\n",
        "\n",
        "Why No Formal Methods?\n",
        "\n",
        "I didn't explicitly use techniques like statistical tests (e.g., Chi-squared), model-based importance (e.g., from a Random Forest), or dimensionality reduction (e.g., PCA) at this stage. My choices were more about common sense and what makes logical sense for describing a restaurant and its reviews. For this dataset, with a manageable number of features whose relevance seemed quite clear, this direct approach felt sufficient. If I were dealing with hundreds or thousands of features, I'd definitely lean on more automated feature selection methods to prevent overfitting and improve efficiency."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Key Important Features*\n",
        "\n",
        "1. Cost: Why it's important: Cost is a fundamental aspect of any restaurant. It heavily influences a customer's expectation and perception of value. For instance, a higher cost often implies a premium experience, better ingredients, or superior ambiance. Conversely, a lower cost might suggest a more casual dining experience. In my analysis, I saw a positive correlation between cost and rating, indicating that as cost increases, so does the rating, implying customers are often satisfied with the value they receive at different price points.\n",
        "\n",
        "2. Rating: Why it's important: As a direct measure of customer satisfaction, Rating is perhaps the most crucial feature. It serves as the primary indicator of a restaurant's success and popularity. When analyzing the merged dataset, Rating becomes a central dependent variable for predictive modeling (e.g., predicting restaurant success) and a key factor for clustering restaurants into quality tiers.\n",
        "\n",
        "3. Number_of_Reviews (Reviewer's Total Reviews) and Reviewer_Followers 📈\n",
        "Why they're important: These features provide insights into the influence and activity of reviewers.\n",
        "\n",
        "  * Number_of_Reviews: This tells me how experienced a reviewer is on the platform. Reviewers with many reviews might be considered more credible or influential.\n",
        "\n",
        "  * Reviewer_Followers: This directly quantifies a reviewer's reach and impact. If a reviewer has many followers, their opinion might hold more weight or their review could attract more attention to a restaurant. My analysis suggested that restaurants reviewed by individuals with more followers tended to receive higher ratings, highlighting the 'influencer effect' in the dining scene.\n",
        "\n",
        "4. Cuisines (and derived features like Total_Cuisine_Count and One-Hot Encoded Cuisines): Why they're important: Cuisines define a restaurant's core offering.\n",
        "\n",
        "  * Total_Cuisine_Count: This engineered feature quantifies the diversity of a restaurant's menu. It helps distinguish highly specialized eateries from those offering a broad range of options. My hypothesis test suggested that a wider variety of cuisines is associated with higher ratings, implying diners appreciate choice.\n",
        "\n",
        "  * One-Hot Encoded Cuisine Features: These binary features (e.g., 'Chinese', 'Italian', 'Biryani') are vital for understanding the specific culinary identity of a restaurant. They allow for the clustering of restaurants based on their food types and are essential for any recommendation system based on cuisine preference.\n",
        "\n",
        "*Why These Features are Important for the Project*\n",
        "\n",
        "These features collectively provide a comprehensive view of a restaurant's characteristics, its performance, and the dynamics of its reviews. They are crucial for:\n",
        "\n",
        "* Clustering: Grouping similar restaurants based on cost, rating, and cuisine profile.\n",
        "\n",
        "* Predictive Modeling: Potentially predicting a restaurant's success or rating based on its attributes and the influence of its reviewers.\n",
        "\n",
        "* Understanding Market Dynamics: Gaining insights into what drives consumer preferences and restaurant performance in the Hyderabad dining scene.\n",
        "\n",
        "* Recommendation Systems: Building systems that can recommend restaurants based on user preferences for cost, cuisine, or desired average rating.\n",
        "\n",
        "By focusing on these features, I can build robust models and derive meaningful insights from the dataset."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Getting symmetric and skew symmetric features from the cplumns\n",
        "symmetric_feature=[]\n",
        "non_symmetric_feature=[]\n",
        "for i in cluster_df.describe().columns:\n",
        "  if abs(cluster_df[i].mean()-cluster_df[i].median())<0.1:\n",
        "    symmetric_feature.append(i)\n",
        "  else:\n",
        "    non_symmetric_feature.append(i)\n",
        "\n",
        "# Getting Symmetric Distributed Features\n",
        "print(\"Symmetric Distributed Features : -\",symmetric_feature)\n",
        "\n",
        "# Getting Skew Symmetric Distributed Features\n",
        "print(\"Skew Symmetric Distributed Features : -\",non_symmetric_feature)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using log transformation to transform Cost as using capping tends to change median and mean\n",
        "cluster_df['Cost'] = np.log1p(cluster_df['Cost'])\n",
        "cluster_dummy['Cost'] = np.log1p(cluster_dummy['Cost'])"
      ],
      "metadata": {
        "id": "jHGo8aNeBstd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import norm # Make sure norm is imported if you're using `fit=norm`\n",
        "\n",
        "# Transform Your data\n",
        "# Ensure 'cluster_df' is defined and has the 'Cost' column.\n",
        "# If not, you'll encounter a NameError.\n",
        "for i, col in enumerate(['Cost']):\n",
        "    plt.figure(figsize=(8, 5)) # It's good practice to create a new figure for each plot in a loop\n",
        "    sns.histplot(cluster_df[col], kde=True, color='#055E85', stat='density') # distplot is deprecated, use histplot/kdeplot\n",
        "    # If you still want the fitted normal distribution, you'd calculate it and plot separately\n",
        "    # x = np.linspace(cluster_df[col].min(), cluster_df[col].max(), 100)\n",
        "    # plt.plot(x, norm.pdf(x, cluster_df[col].mean(), cluster_df[col].std()), color='green', linestyle='--', label='Normal Fit')\n",
        "\n",
        "    feature = cluster_df[col]\n",
        "    plt.axvline(feature.mean(), color='#ff033e', linestyle='dashed', linewidth=3, label='mean');  # red\n",
        "    plt.axvline(feature.median(), color='#A020F0', linestyle='dashed', linewidth=3, label='median'); # cyan\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left') # Corrected 'loc' from 'up' to 'upper left'\n",
        "    plt.title(f'{col.title()} Distribution'); # Added \"Distribution\" to title for clarity\n",
        "    plt.tight_layout();\n",
        "    plt.show() # Display the plot"
      ],
      "metadata": {
        "id": "zx8oBeDUBwEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the data for the Cost feature needed to be transformed. I used log transformation (specifically np.log1p) for this.\n",
        "\n",
        "*Data Transformation: Log Transformation on 'Cost'*\n",
        "\n",
        "Why Transformation Was Needed\n",
        "\n",
        "The initial analysis identified Cost as a skew-symmetric feature. This means its distribution was heavily skewed (likely to the right, with a long tail of higher costs), as indicated by the mean being significantly different from the median (the condition abs(cluster_df[i].mean()-cluster_df[i].median())<0.1 suggests it failed this symmetry test).\n",
        "\n",
        "* Impact of Skewness on Models: Many machine learning algorithms (especially linear models or those assuming normality) perform poorly or yield biased results when input features are highly skewed. Skewed data can also lead to issues with outlier detection and influence model convergence.\n",
        "\n",
        "Why Log Transformation (np.log1p) Was Used\n",
        "\n",
        "* Normalizing Skewed Distributions: Logarithmic transformations are highly effective at reducing right-skewness in data. By compressing the range of values, they make the distribution more symmetrical and closer to a normal distribution.\n",
        "\n",
        "* Handling Zero Values: I used np.log1p (which computes log(1+x)) specifically because the Cost column might contain zero values. A standard np.log() function would produce an error or infinity for zero, whereas np.log1p handles it gracefully, returning 0 for* an input of 0.\n",
        "\n",
        "* Outlier Treatment Alternative: As your comment mentions, \"using capping tends to change median and mean.\" While capping outliers (as done previously with the IQR method) can manage extreme values, it doesn't fundamentally change the shape of the distribution. Log transformation, however, directly addresses the skewness, often making the distribution more amenable to statistical analysis and modeling. It effectively \"pulls in\" high values, mitigating their extreme influence without simply clipping them.\n",
        "\n",
        "By applying the log transformation to Cost, I aimed to make its distribution more normal-like, which often leads to better performance for machine learning models that expect normally distributed inputs and helps in satisfying the assumptions of various statistical tests."
      ],
      "metadata": {
        "id": "cDFA_fKKB12z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "cluster_dummy.sample(5)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalizing numerical columns\n",
        "numerical_cols = ['Cost','Total_Cuisine_Count','Average_Rating']\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(cluster_dummy[numerical_cols])\n",
        "scaled_df = cluster_dummy.copy()\n",
        "scaled_df[numerical_cols] = scaler.transform(cluster_dummy[numerical_cols])"
      ],
      "metadata": {
        "id": "lnLreVUfDwnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Standardization (specifically StandardScaler from scikit-learn) to scale my data.\n",
        "\n",
        "Data Scaling Method\n",
        "\n",
        "Standardization (Z-score Normalization): I applied StandardScaler to the numerical columns: Cost, Total_Cuisine_Count, and Average_Rating.\n",
        "\n",
        "How Standardization Works:\n",
        "\n",
        "Standardization (also known as Z-score normalization) transforms data such that it has a mean of 0 and a standard deviation of 1. For each data point (x), it's calculated using the formula:\n",
        "\n",
        "      z = (x−μ​) / σ\n",
        "\n",
        "where:\n",
        "\n",
        "* μ is the mean of the feature\n",
        "\n",
        "* σ is the standard deviation of the feature\n",
        "\n",
        "Why Standardization Was Used:\n",
        "\n",
        "* Handles Varying Scales: Features like Cost might have values ranging in hundreds or thousands, while Total_Cuisine_Count might range from 1 to 10, and Average_Rating from 1 to 5. Without scaling, features with larger numerical ranges can disproportionately influence machine learning algorithms, simply because their absolute values are bigger. Standardization brings all features to a comparable scale, preventing this dominance.\n",
        "\n",
        "* Benefits Distance-Based Algorithms: Many machine learning algorithms, particularly those that rely on distance calculations (like K-Means Clustering, which you're likely preparing for with cluster_dummy), K-Nearest Neighbors (KNN), or Support Vector Machines (SVMs), are very sensitive to the scale of the input features. If features are on different scales, the distances calculated will be heavily skewed by the features with larger magnitudes. Standardization ensures that each feature contributes equally to the distance calculation.\n",
        "\n",
        "* Improves Model Convergence: For optimization algorithms used in models like Linear Regression, Logistic Regression, or Neural Networks, standardization can lead to faster and more stable convergence by providing a more uniform landscape for the optimization process.\n",
        "\n",
        "* Assumes No Specific Distribution (Relatively Robust to Outliers if pre-treated): While it doesn't bound values to a specific range (like Min-Max Scaling), Standardization works well even if the data isn't normally distributed, especially after outlier treatment (which I already performed on Cost). It's less affected by extreme values than methods like Min-Max scaling, provided those outliers have been managed.\n",
        "\n",
        "In summary, standardizing my numerical features ensures that my subsequent machine learning models (especially those focused on clustering, as indicated by cluster_dummy) treat all features with equal importance based on their information content, not just their raw magnitude."
      ],
      "metadata": {
        "id": "B53kCZJ6EFqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the current state of your cluster_dummy DataFrame, yes, I think dimensionality reduction is needed.\n",
        "\n",
        "Why Dimensionality Reduction is Important 📉\n",
        "\n",
        "Your cluster_dummy DataFrame, intended for clustering, currently has 48 columns. A significant portion of these are one-hot encoded cuisine features. While necessary for representing culinary variety, having 40+ binary cuisine columns can lead to several issues:\n",
        "\n",
        "1. The Curse of Dimensionality: As the number of features (dimensions) increases, the data becomes extremely sparse. In high-dimensional spaces, data points become very far apart, making it difficult for distance-based algorithms like K-Means (which you're likely setting up for) to effectively group similar items. Clusters become less meaningful, and every point can appear to be an \"outlier\" relative to others.\n",
        "\n",
        "2. Increased Computational Cost: More dimensions mean more calculations. Training models, especially clustering algorithms, becomes significantly slower and requires more memory.\n",
        "\n",
        "3. Overfitting (for Supervised Models): While clustering is unsupervised, if you later use these features for a supervised learning task (like predicting ratings based on restaurant attributes), too many dimensions can lead to models that fit the noise in the training data rather than the underlying patterns.\n",
        "\n",
        "4. Reduced Interpretability: It's much harder to visualize or understand relationships in 48 dimensions compared to, say, 2, 3, or even 10 principal components.\n",
        "\n",
        "5. Multicollinearity: Many of your cuisine columns might be highly correlated (e.g., if \"North Indian\" often appears with \"Biryani\"). This multicollinearity can destabilize some algorithms and make feature importance harder to interpret.\n",
        "\n",
        "How it Specifically Applies to Your Data\n",
        "\n",
        "You have Cost, Average_Rating, Total_Cuisine_Count, and then 45 individual cuisine type columns. Many restaurants might only offer a few cuisines, leading to a high percentage of zeros in these one-hot encoded columns. This sparsity, combined with the sheer number of these features, makes your dataset a strong candidate for dimensionality reduction."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction (If needed)\n",
        "#applying pca\n",
        "#setting restaurant feature as index as it still had categorial value\n",
        "scaled_df.set_index(['Restaurant'],inplace=True)\n",
        "features = scaled_df.columns\n",
        "# features = features.drop('Restaurant')\n",
        "# create an instance of PCA\n",
        "pca = PCA()\n",
        "\n",
        "# fit PCA on features\n",
        "pca.fit(scaled_df[features])"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#explained variance v/s no. of components\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker ='o', color = 'orange')\n",
        "plt.xlabel('number of components',size = 15, color = 'red')\n",
        "plt.ylabel('cumulative explained variance',size = 14, color = 'blue')\n",
        "plt.title('Variance v/s No. of Components',size = 20, color = 'green')\n",
        "plt.xlim([0, 8])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PHRjjPKfFiOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using n_component as 3\n",
        "pca = PCA(n_components=3)\n",
        "\n",
        "# fit PCA on features\n",
        "pca.fit(scaled_df[features])\n",
        "\n",
        "# explained variance ratio of each principal component\n",
        "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "# variance explained by three components\n",
        "print('Cumulative variance explained by 3 principal components: {:.2%}'.format(\n",
        "                                        np.sum(pca.explained_variance_ratio_)))\n",
        "\n",
        "# transform data to principal component space\n",
        "df_pca = pca.transform(scaled_df[features])"
      ],
      "metadata": {
        "id": "59ufiSQgFsS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shape\n",
        "print(\"original shape: \", scaled_df.shape)\n",
        "print(\"transformed shape:\", df_pca.shape)"
      ],
      "metadata": {
        "id": "FnGukGeaFvxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Dimensionality Reduction Technique Used*\n",
        "\n",
        "I used Principal Component Analysis (PCA) as my dimensionality reduction technique.\n",
        "\n",
        "How I used it:\n",
        "\n",
        "1. I initialized PCA() without specifying n_components first to plot the explained variance ratio. This plot (Variance v/s No. of Components) showed me how much of the total variance in the data could be explained by an increasing number of principal components.\n",
        "\n",
        "2. Based on the plot, I observed that the first few components captured a significant portion of the variance. I specifically chose n_components=3 for the final transformation because, as the output showed, these 3 principal components collectively explained 62.71% of the total variance in the data.\n",
        "\n",
        "3. Finally, I used pca.transform() to project my scaled_df (original shape: (100, 47)) into the new, lower-dimensional space, resulting in df_pca with a shape of (100, 3).\n",
        "\n",
        "Why PCA was chosen:\n",
        "\n",
        "1. Feature Extraction: PCA is a feature extraction technique, meaning it creates new, synthetic features (principal components) that are linear combinations of the original features. This is beneficial when you want to retain as much information as possible from the original set but in a more compact form, rather than just discarding features (like in feature selection).\n",
        "\n",
        "2. Variance Maximization: PCA aims to find the directions (principal components) along which the data has the highest variance. By selecting components that explain most of the variance, I ensured that the most important patterns and information in my original 47 features were largely preserved in just 3 components.\n",
        "\n",
        "3. Orthogonal Components: The principal components are statistically uncorrelated (orthogonal). This helps in dealing with multicollinearity present in the original features (especially the cuisine dummies).\n",
        "\n",
        "4. Suitability for Clustering: PCA is a very common and effective preprocessing step for clustering. By reducing the noise and redundancy, it helps clustering algorithms identify more distinct and meaningful groups. It specifically addresses the \"curse of dimensionality\" by condensing the high-dimensional data into a lower-dimensional representation while preserving the most significant variation."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# for sentiment analysis using sentiment_df dataframe\n",
        "X = X_tfidf #from text vectorization\n",
        "y = sentiment_df['Sentiment']"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.shape"
      ],
      "metadata": {
        "id": "HI1O-qvRHRuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spliting test train\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
        "\n",
        "# describes info about train and test set\n",
        "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
        "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
        "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
        "print(\"Number transactions y_test dataset: \", y_test.shape)"
      ],
      "metadata": {
        "id": "TWerHIcQHU-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I've used an 80/20 split for the training and testing datasets.\n",
        "\n",
        "Data Splitting Ratio: I split the data for sentiment analysis into:\n",
        "\n",
        "80% for the training set (X_train, y_train)\n",
        "\n",
        "20% for the testing set (X_test, y_test)\n",
        "\n",
        "Why this Ratio?\n",
        "\n",
        "An 80/20 split is a very common and generally robust choice in machine learning for several reasons:\n",
        "\n",
        "1. Sufficient Training Data: With 80% of the data, the model has a large enough portion to learn complex patterns and relationships within the text data (represented by X_tfidf) and their corresponding sentiments (y). This helps in building a well-generalized model.\n",
        "\n",
        "2. Representative Testing Data: The remaining 20% provides a decent-sized, unseen dataset to evaluate the model's performance realistically. It's large enough to give a statistically meaningful assessment of how well the model generalizes to new, unobserved reviews. A smaller test set might lead to a less reliable evaluation.\n",
        "\n",
        "3. Balance: It strikes a good balance between providing ample data for training and reserving enough data for unbiased evaluation. While other ratios like 70/30 or 75/25 are also common, 80/20 often serves as a good default for many standard datasets."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, I think the dataset is likely imbalanced, especially if the Sentiment column, derived from Rating, represents discrete sentiment categories (e.g., Positive, Neutral, Negative).\n",
        "\n",
        "Why It's Likely Imbalanced\n",
        "\n",
        "1. Nature of Review Data: User review datasets, particularly those involving star ratings (like your Rating column), are very commonly imbalanced. People are often more motivated to leave a review when they have had a very positive or a very negative experience. Mediocre or \"average\" experiences tend to be reviewed less frequently. Moreover, most products/services, to remain viable, must maintain a generally satisfactory performance, leading to a natural skew towards higher ratings.\n",
        "\n",
        "2. Observed Rating Distributions: In many real-world scenarios, review ratings often follow a J-shaped or left-skewed distribution when plotted, meaning there's a higher frequency of 4-star and 5-star reviews, a moderate frequency of 1-star reviews, and fewer 2-star and 3-star reviews.\n",
        "\n",
        "  * If your Sentiment categories are mapped from these ratings (e.g., 4-5 stars = Positive, 2-3 stars = Neutral, 1 star = Negative), then the \"Positive\" class will almost certainly have a significantly higher number of instances than the \"Neutral\" or \"Negative\" classes.\n",
        "\n",
        "Implications of Imbalance\n",
        "\n",
        "An imbalanced dataset can pose significant challenges for machine learning models, particularly in classification tasks like sentiment analysis:\n",
        "\n",
        "1. Biased Model Performance: A model trained on an imbalanced dataset tends to be biased towards the majority class. It might achieve high overall accuracy by simply predicting the majority class for most instances, but it will perform very poorly on the minority classes. For example, if 90% of reviews are positive, a model that predicts \"Positive\" for every review would achieve 90% accuracy, but it would be useless for identifying negative or neutral sentiment.\n",
        "\n",
        "2. Poor Generalization for Minority Classes: The model might fail to learn the distinctive patterns of the minority classes due to insufficient examples. This leads to poor recall and precision for the underrepresented sentiments, which are often the most important ones to correctly identify (e.g., recognizing negative feedback to address customer issues).\n",
        "\n",
        "3. Misleading Evaluation Metrics: Metrics like accuracy can be deceptive. It's crucial to use other metrics like Precision, Recall, F1-score, and AUC-ROC which provide a more nuanced view of performance across all classes, especially minority ones.\n",
        "\n",
        "To definitively confirm the imbalance, I would need to perform a value_counts() on the sentiment_df['Sentiment'] column. If confirmed, strategies like oversampling (SMOTE), undersampling, or using class weights during model training would be critical next steps to handle this imbalance effectively."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#getting the value count for target class\n",
        "vc = sentiment_df.Sentiment.value_counts().reset_index().rename(columns =\n",
        "            {'index':'Sentiment','Sentiment':'Count'})"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining majority and minority class value\n",
        "majority_class = vc.Count[0]\n",
        "minority_class = vc.Count[1]"
      ],
      "metadata": {
        "id": "tWytGVqoIZ-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating cir value for checking class imbalance\n",
        "CIR = majority_class / minority_class\n",
        "CIR"
      ],
      "metadata": {
        "id": "v2emx-2DIdAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependant Variable Column Visualization\n",
        "sentiment_df['Sentiment'].value_counts().plot(kind='pie',\n",
        "                              figsize=(15,6),\n",
        "                               autopct=\"%1.1f%%\",\n",
        "                               startangle=90,\n",
        "                               shadow=True,\n",
        "                               labels=['Positive Sentiment','Negative Sentiment'],\n",
        "                               colors=['red','blue'],\n",
        "                               explode=[0.01,0.02]\n",
        "                              )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4xTq-DIHIheF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling Imbalance Decision\n",
        "\n",
        "Decision: I chose not to perform any under-sampling or over-sampling techniques to treat the class imbalance.\n",
        "\n",
        "Reasoning:\n",
        "\n",
        "1. Slight Imbalance: While the Class Imbalance Ratio (CIR) of 1.73 indicates that the majority class has about 1.73 times more observations than the minority class, this is considered a slight imbalance.\n",
        "\n",
        "2. Not Critically Severe: For many machine learning algorithms, particularly robust ones like tree-based models (e.g., Random Forest, Gradient Boosting) or those that can handle class weights, a CIR of 1.73 might not severely degrade performance. The benefits of resampling (which can sometimes introduce noise or lose information) might not outweigh the costs in this case.\n",
        "\n",
        "By making this decision, I'm essentially trusting that the chosen machine learning model will be robust enough to handle this level of imbalance, or that the cost of misclassifying the minority class is not high enough to warrant explicit balancing techniques."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "#importing kmeans\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Within Cluster Sum of Squared Errors(WCSS) for different values of k\n",
        "wcss=[]\n",
        "for i in range(1,11):\n",
        "    km=KMeans(n_clusters=i,random_state = 20)\n",
        "    km.fit(df_pca)\n",
        "    wcss.append(km.inertia_)"
      ],
      "metadata": {
        "id": "iezFcGM3JlEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#elbow curve\n",
        "plt.plot(range(1,11),wcss)\n",
        "plt.plot(range(1,11),wcss, linewidth=2, color=\"red\", marker =\"o\")\n",
        "plt.xlabel(\"K Value\", size = 20, color = 'purple')\n",
        "plt.xticks(np.arange(1,11,1))\n",
        "plt.ylabel(\"WCSS\", size = 20, color = 'green')\n",
        "plt.title('Elbow Curve', size = 20, color = 'blue')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tkNU3J4mJpFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans # Make sure KMeans is imported\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# candidates for the number of cluster\n",
        "parameters = list(range(2,10))\n",
        "#parameters\n",
        "parameter_grid = ParameterGrid({'n_clusters': parameters})\n",
        "best_score = -1\n",
        "\n",
        "#visualizing Silhouette Score for individual clusters and the clusters made\n",
        "for n_clusters in parameters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # 1st subplot is the silhouette plot\n",
        "    # silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(df_pca) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=10, n_init='auto') # Added n_init='auto' to suppress future warning\n",
        "    cluster_labels = clusterer.fit_predict(df_pca)\n",
        "\n",
        "    # silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(df_pca, cluster_labels)\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"average silhouette_score is :\", silhouette_avg)\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(df_pca, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"Silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"Silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "    ax2.scatter(df_pca[:, 0], df_pca[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                c=colors, edgecolor='k')\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "    # Corrected line: Use f-string to format the cluster number as a text marker\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker=f'${i}$', alpha=1, # Corrected part\n",
        "                    s=50, edgecolor='k')\n",
        "\n",
        "    ax2.set_title(\"Visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_clusters),\n",
        "                 fontsize=14, fontweight='bold')\n",
        "    plt.show() # Added to display each plot"
      ],
      "metadata": {
        "id": "3qbsZRvsJ6wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vizualizing the clusters and the datapoints in each clusters\n",
        "plt.figure(figsize = (10,6), dpi = 120)\n",
        "\n",
        "kmeans= KMeans(n_clusters = 5, init= 'k-means++', random_state = 42)\n",
        "kmeans.fit(df_pca)\n",
        "\n",
        "#predict the labels of clusters.\n",
        "label = kmeans.fit_predict(df_pca)\n",
        "#Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        "\n",
        "#plotting the results:\n",
        "for i in unique_labels:\n",
        "    plt.scatter(df_pca[label == i , 0] , df_pca[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irV87rN3LEzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#making df for pca\n",
        "kmeans_pca_df = pd.DataFrame(df_pca,columns=['PC1','PC2','PC3'],index=scaled_df.index)\n",
        "kmeans_pca_df[\"label\"] = label\n",
        "kmeans_pca_df.sample(2)"
      ],
      "metadata": {
        "id": "qM0rUZ0xLJ50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#joining the cluster labels to names dataframe\n",
        "cluster_dummy.set_index(['Restaurant'],inplace=True)\n",
        "cluster_dummy = cluster_dummy.join(kmeans_pca_df['label'])\n",
        "cluster_dummy.sample(2)"
      ],
      "metadata": {
        "id": "F5zPZCHCLQTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing back cost value to original from log1p done during transformation\n",
        "cluster_dummy['Cost'] = np.expm1(cluster_dummy['Cost'])\n",
        "cluster_dummy.sample(2)"
      ],
      "metadata": {
        "id": "dr8qBYQ7LXND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating df to store cluster data\n",
        "clustering_result = cluster_dummy.copy().reset_index()\n",
        "clustering_result = hotel[['Restaurant','Cuisines']].merge(clustering_result[['Restaurant','Cost',\n",
        "                  'Average_Rating',\t'Total_Cuisine_Count','label']], on = 'Restaurant')\n",
        "clustering_result.head()"
      ],
      "metadata": {
        "id": "YIkubmL7Leaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting content in each cluster\n",
        "cluster_count = cluster_dummy['label'].value_counts().reset_index().rename(\n",
        "    columns={'index':'label','label':'Total_Restaurant'}).sort_values(by='Total_Restaurant')\n",
        "cluster_count"
      ],
      "metadata": {
        "id": "Xv96emWULk1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new df for checkign cuising in each cluster\n",
        "new_cluster_df = clustering_result.copy()\n",
        "new_cluster_df['Cuisines'] = new_cluster_df['Cuisines'].str.split(',')\n",
        "new_cluster_df = new_cluster_df.explode('Cuisines')\n",
        "#removing extra trailing space from cuisines after exploded\n",
        "new_cluster_df['Cuisines'] = new_cluster_df['Cuisines'].apply(lambda x: x.strip())\n",
        "new_cluster_df.sample(5)"
      ],
      "metadata": {
        "id": "JO8-79ndLwXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing cuisine list for each cluster\n",
        "for cluster in new_cluster_df['label'].unique().tolist():\n",
        "  print('Cuisine List for Cluster :', cluster,'\\n')\n",
        "  print(new_cluster_df[new_cluster_df[\"label\"]== cluster]['Cuisines'].unique(),'\\n')\n",
        "  print('='*120)"
      ],
      "metadata": {
        "id": "zzjS7mBoL5Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the K-Means clustering algorithm as my machine learning model.\n",
        "\n",
        "ML Model: K-Means Clustering 📊\n",
        "\n",
        "I've implemented K-Means, an unsupervised machine learning algorithm, for clustering the restaurants based on their transformed features (from PCA).\n",
        "\n",
        "How K-Means Works:\n",
        "\n",
        "K-Means aims to partition n observations into k clusters, where each observation belongs to the cluster with the nearest mean (centroid).\n",
        "\n",
        "1. Initialization: It randomly initializes k centroids.\n",
        "\n",
        "2. Assignment Step: Each data point is assigned to the nearest centroid, forming k clusters.\n",
        "\n",
        "3. Update Step: The centroids are re-calculated as the mean of all data points assigned to that cluster.\n",
        "\n",
        "4. Iteration: Steps 2 and 3 are repeated until the centroids no longer move significantly or a maximum number of iterations is reached.\n",
        "\n",
        "Model Implementation Steps:\n",
        "\n",
        "1. Determining Optimal k (Number of Clusters): I used two methods to find the optimal number of clusters:\n",
        "\n",
        "* Elbow Method (using WCSS - Within-Cluster Sum of Squares): I calculated the WCSS for k values ranging from 1 to 10. WCSS measures the sum of squared distances between each point and its cluster centroid. The \"elbow\" point in the plot, where the rate of decrease in WCSS sharply changes, suggests an optimal k.\n",
        "\n",
        "  * Observation from Elbow Curve: While not perfectly sharp, the elbow curve shows a significant drop up to k=3 or k=4, and then the rate of decrease slows. It suggests a value around 3 or 4 might be reasonable.\n",
        "\n",
        "* Silhouette Score Analysis: I calculated the average silhouette score for k values from 2 to 9. The silhouette score measures how similar an object is to its own cluster compared to other clusters. Higher values indicate better-defined and more separated clusters.\n",
        "\n",
        "  * Observation from Silhouette Scores:\n",
        "\n",
        "    * For n_clusters = 2, the average silhouette score is: 0.31357\n",
        "\n",
        "    * For n_clusters = 3, the average silhouette score is: 0.29742\n",
        "\n",
        "    * For n_clusters = 4, the average silhouette score is: 0.31274\n",
        "\n",
        "    * For n_clusters = 5, the average silhouette score is: 0.30244\n",
        "\n",
        "    * For n_clusters = 6, the average silhouette score is: 0.31674 (Highest)\n",
        "\n",
        "    * For n_clusters = 7, the average silhouette score is: 0.30965\n",
        "\n",
        "    * For n_clusters = 8, the average silhouette score is: 0.29778\n",
        "\n",
        "    * For n_clusters = 9, the average silhouette score is: 0.29957\n",
        "\n",
        "* Based on the silhouette scores, k=6 yields the highest average silhouette score, suggesting it's the best choice for cluster separation and cohesion among the tested values.\n",
        "\n",
        "2. Final K-Means Model: Although the silhouette score suggested 6, you then proceeded to fit K-Means with n_clusters = 5 (kmeans= KMeans(n_clusters = 5, init= 'k-means++', random_state = 42)). This might be a deliberate choice based on domain interpretability or other factors not explicitly shown.\n",
        "\n",
        "3. Cluster Assignment: The kmeans.fit_predict(df_pca) step assigns each restaurant (represented by its PCA-transformed features) to one of the 5 (or 6, depending on the final k choice) clusters.\n",
        "\n",
        "4. Cluster Visualization: The plt.scatter plot shows the data points colored by their assigned cluster in the 2D PCA space (using PC1 and PC2), providing a visual representation of the clusters formed. The cluster centers are also plotted.\n",
        "\n",
        "5. Adding Cluster Labels to DataFrame: The label (cluster ID) is then added back to kmeans_pca_df and subsequently joined with cluster_dummy (after converting Cost back from log1p), enabling further analysis of what defines each cluster.\n",
        "\n",
        "Performance Evaluation Metric: Silhouette Score Chart\n",
        "The Silhouette Score is the primary evaluation metric used here for K-Means clustering.\n",
        "\n",
        "Export to Sheets\n",
        "As seen, n_clusters = 6 yields the highest average silhouette score of 0.31674. This indicates that, among the tested k values, 6 clusters provide the best separation between clusters and cohesion within clusters, making it the most optimal choice according to this metric.\n",
        "\n",
        "Despite the silhouette score suggesting 6 clusters, your final K-Means model for visualization and DataFrame merging was set to n_clusters=5. If the goal is to choose the best k based purely on the silhouette score, then k=6 would be the data-driven choice from this evaluation."
      ],
      "metadata": {
        "id": "tRB1NhwNM7_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing module for hierarchial clustering and vizualizing dendograms\n",
        "import scipy.cluster.hierarchy as sch\n",
        "plt.figure(figsize=(12,5))\n",
        "dendrogram = sch.dendrogram(sch.linkage(df_pca, method = 'ward'),orientation='top',\n",
        "            distance_sort='descending',\n",
        "            show_leaf_counts=True)\n",
        "\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Restaurants')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np # Ensure numpy is imported if df_pca is a numpy array\n",
        "\n",
        "range_n_clusters = [2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
        "for n_clusters in range_n_clusters:\n",
        "    # Removed affinity = 'euclidean' because linkage = 'ward' automatically uses euclidean distance.\n",
        "    hc = AgglomerativeClustering(n_clusters = n_clusters, linkage = 'ward')\n",
        "    y_hc = hc.fit_predict(df_pca)\n",
        "    score = silhouette_score(df_pca, y_hc)\n",
        "    print(\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))"
      ],
      "metadata": {
        "id": "xc0QKZXFONCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# agglomerative clustering\n",
        "from numpy import unique\n",
        "from numpy import where\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# define the model\n",
        "model = AgglomerativeClustering(n_clusters = 5)      #n_clusters=5\n",
        "# fit model and predict clusters\n",
        "y_hc = model.fit_predict(df_pca)\n",
        "# retrieve unique clusters\n",
        "clusters = unique(y_hc)\n",
        "# create scatter plot for samples from each cluster\n",
        "for cluster in clusters:\n",
        "\t# get row indexes for samples with this cluster\n",
        "\trow_ix = where(y_hc == cluster)\n",
        "\t# create scatter of these samples\n",
        "\tplt.scatter(df_pca[row_ix, 0], df_pca[row_ix, 1])\n",
        "# show the plot\n",
        "plt.show()\n",
        "#Evaluation\n",
        "\n",
        "#Silhouette Coefficient\n",
        "print(\"Silhouette Coefficient: %0.3f\"%silhouette_score(df_pca,y_hc, metric='euclidean'))\n",
        "\n",
        "#davies_bouldin_score of our clusters\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "davies_bouldin_score(df_pca, y_hc)\n",
        "print(\"davies_bouldin_score %0.3f\"%davies_bouldin_score(df_pca, y_hc))"
      ],
      "metadata": {
        "id": "6napzz-YPAwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new colummn for predicting cluster using hierarcial clsutering\n",
        "clustering_result['label_hr'] = y_hc"
      ],
      "metadata": {
        "id": "KulJA9UzPE61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustering_result.sample(5)"
      ],
      "metadata": {
        "id": "I4Bug-dqPH1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import LatentDirichletAllocation # Import LatentDirichletAllocation\n",
        "\n",
        "topic_range = range(2, 11)\n",
        "silhouette_scores = []\n",
        "\n",
        "for n_components in topic_range:\n",
        "    # Added random_state for reproducibility. It's good practice for algorithms with random initialization.\n",
        "    lda = LatentDirichletAllocation(n_components=n_components, random_state=42)\n",
        "    lda.fit(X)\n",
        "    labels = lda.transform(X).argmax(axis=1)\n",
        "    silhouette_scores.append(silhouette_score(X, labels))"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting silhouette score\n",
        "plt.plot(topic_range, silhouette_scores, marker ='o', color='red')\n",
        "plt.xlabel('Number of Topics', size = 15, color = 'green')\n",
        "plt.ylabel('Silhouette Score', size = 15, color = 'blue')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HMchOYTfRlr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA model\n",
        "lda = LatentDirichletAllocation(n_components=4)\n",
        "lda.fit(X)"
      ],
      "metadata": {
        "id": "_8JuymozRrg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.lda_model # This module is typically used for sklearn's LDA models\n",
        "\n",
        "pyLDAvis.enable_notebook()"
      ],
      "metadata": {
        "id": "MdtPuInoR0bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis.lda_model # Corrected import\n",
        "\n",
        "# ploting the clusters top 30 terms\n",
        "# Assuming 'lda' is your fitted LatentDirichletAllocation model,\n",
        "# 'X' is your document-term matrix (e.g., TF-IDF or CountVectorizer output),\n",
        "# and 'vectorizer' is your fitted TfidfVectorizer or CountVectorizer.\n",
        "lda_pyLDAvis = pyLDAvis.lda_model.prepare(lda, X, vectorizer, mds='tsne')\n",
        "lda_pyLDAvis"
      ],
      "metadata": {
        "id": "yiL2O9QXTHGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating copy to store predicted sentiments\n",
        "review_sentiment_prediction = review[review_df.columns.to_list()].copy()\n",
        "review_sentiment_prediction.head()"
      ],
      "metadata": {
        "id": "CJSbD8YHTdoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting the sentiments and storing in a feature\n",
        "topic_results = lda.transform(X)\n",
        "review_sentiment_prediction['Prediction'] = topic_results.argmax(axis=1)\n",
        "review_sentiment_prediction.sample(5)"
      ],
      "metadata": {
        "id": "NX7gy7QJTiU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Define the number of words to include in the word cloud\n",
        "N = 100\n",
        "\n",
        "# Create a list of strings for each topic\n",
        "topic_text = []\n",
        "for index, topic in enumerate(lda.components_):\n",
        "    # Corrected: Use get_feature_names_out() instead of the deprecated get_feature_names()\n",
        "    topic_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-N:]]\n",
        "    topic_text.append(\" \".join(topic_words))\n",
        "\n",
        "# Create a word cloud for each topic\n",
        "for i in range(len(topic_text)):\n",
        "    print(f'TOP 100 WORDS FOR TOPIC #{i}')\n",
        "    wordcloud = WordCloud(background_color=\"black\", colormap='rainbow').generate(topic_text[i])\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    print('='*120)"
      ],
      "metadata": {
        "id": "HqgS4pYdToyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentiment in review_sentiment_prediction['Prediction'].unique().tolist():\n",
        "  print('Prediction = ',sentiment,'\\n')\n",
        "  print(review_sentiment_prediction[review_sentiment_prediction['Prediction'] ==\n",
        "        sentiment]['Rating'].value_counts())\n",
        "  print('='*120)"
      ],
      "metadata": {
        "id": "muFNWX3fUVBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining function to calculate score\n",
        "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
        "from tabulate import tabulate\n",
        "import itertools\n",
        "\n",
        "\n",
        "#calculating score\n",
        "def calculate_scores(model, X_train, y_train, X_test, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    # Get the confusion matrix for both train and test\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.imshow(cm, cmap='Wistia')\n",
        "\n",
        "    # Add labels to the plot\n",
        "    class_names = [\"Positive\", \"Negative\"]\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "\n",
        "    # Add values inside the confusion matrix\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                horizontalalignment=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    # Add a title and x and y labels\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.ylabel('True label')\n",
        "\n",
        "    plt.show()\n",
        "    print(cm)\n",
        "    return roc_auc, f1, accuracy, precision, recall\n",
        "\n",
        "#printing result\n",
        "def print_table(model, X_train, y_train, X_test, y_test):\n",
        "    roc_auc, f1, accuracy, precision, recall = calculate_scores(model, X_train, y_train, X_test, y_test)\n",
        "    table = [[\"ROC AUC\", roc_auc], [\"Precision\", precision],\n",
        "             [\"Recall\", recall], [\"F1\", f1], [\"Accuracy\", accuracy]]\n",
        "    print(tabulate(table, headers=[\"Metric\", \"Score\"]))"
      ],
      "metadata": {
        "id": "8R_HnYvzUk4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#logisctic regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# create and fit the model\n",
        "clf = LogisticRegression()"
      ],
      "metadata": {
        "id": "LW5KLSR8UrR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#XgBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "#create and fit the model\n",
        "xgb = XGBClassifier()"
      ],
      "metadata": {
        "id": "J6xqs6unUvuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart for logistic regression\n",
        "# printing result\n",
        "print_table(clf, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "nK4bY2zgU5z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart for XgBoost\n",
        "# printing result\n",
        "print_table(xgb, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "QG66dO3IU-iS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "#logistic regression\n",
        "# finding the best parameters for LogisticRegression by gridsearchcv\n",
        "param_dict = {'C': [0.1,1,10,100,1000],'penalty': ['l1', 'l2'],'max_iter':[1000]}\n",
        "clf_grid = GridSearchCV(clf, param_dict,n_jobs=-1, cv=5, verbose = 5,scoring='recall')"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing result\n",
        "print_table(clf_grid, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "Q1CQ4CrpVIUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best parameters for XGBRegressor by gridsearchcv\n",
        "xgb_param={'n_estimators': [100,125,150],'max_depth': [7,10,15],'criterion': ['entropy']}\n",
        "xgb_grid=GridSearchCV(estimator=xgb,param_grid = xgb_param,cv=3,scoring='recall',verbose=5,n_jobs = -1)"
      ],
      "metadata": {
        "id": "4c4TnjIpVNCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing result for gridsearch Xgb\n",
        "print_table(xgb_grid, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "RoXXasIZVQu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd  # Import pandas\n",
        "from sklearn.metrics import roc_curve, roc_auc_score # Ensure roc_auc_score is imported\n",
        "\n",
        "# finding the best parameters for all the models\n",
        "# Assuming clf_grid and xgb_grid are already fitted GridSearchCV objects\n",
        "log_reg_best = clf_grid.best_estimator_\n",
        "xgbc_best = xgb_grid.best_estimator_\n",
        "\n",
        "# predicting the sentiment by all models\n",
        "# X_test needs to be defined from your earlier sentiment analysis split\n",
        "y_preds_proba_lr = log_reg_best.predict_proba(X_test)[:, 1] # Simplified slicing\n",
        "y_preds_proba_xgbc = xgbc_best.predict_proba(X_test)[:, 1] # Simplified slicing\n",
        "\n",
        "classifiers_proba = [(log_reg_best, y_preds_proba_lr),\n",
        "                    (xgbc_best, y_preds_proba_xgbc)]\n",
        "\n",
        "# Define a result table as a DataFrame\n",
        "# Initialize an empty list to store dictionaries, then concatenate\n",
        "results = []\n",
        "\n",
        "# Train the models and record the results\n",
        "for pair in classifiers_proba:\n",
        "    fpr, tpr, _ = roc_curve(y_test,  pair[1])\n",
        "    auc = roc_auc_score(y_test, pair[1])\n",
        "\n",
        "    results.append({\n",
        "        'classifiers': pair[0].__class__.__name__,\n",
        "        'fpr': fpr,\n",
        "        'tpr': tpr,\n",
        "        'auc': auc\n",
        "    })\n",
        "\n",
        "# Convert list of dictionaries to DataFrame\n",
        "result_table = pd.DataFrame(results)\n",
        "\n",
        "# Set name of the classifiers as index labels\n",
        "result_table.set_index('classifiers', inplace=True)\n",
        "\n",
        "# ploting the roc auc curve for all models\n",
        "fig = plt.figure(figsize=(10,6))\n",
        "for i in result_table.index:\n",
        "    plt.plot(result_table.loc[i]['fpr'],\n",
        "             result_table.loc[i]['tpr'],\n",
        "             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n",
        "\n",
        "plt.plot([0,1], [0,1],'r--') # Plotting the random classifier line\n",
        "\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=15)\n",
        "plt.ylabel(\"True Positive Rate\", fontsize=15)\n",
        "plt.title('ROC AUC Curve', fontweight='bold', fontsize=15)\n",
        "plt.legend(prop={'size':13}, loc='lower right')\n",
        "plt.grid(True) # Added grid for better readability\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-XnKgpEEVaA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I've used Grid Search Cross-Validation (GridSearchCV) for hyperparameter optimization for both Logistic Regression and XGBoost models.\n",
        "\n",
        "Hyperparameter Optimization Technique: Grid Search Cross-Validation (GridSearchCV)\n",
        "\n",
        "How GridSearchCV Works:\n",
        "\n",
        "GridSearchCV exhaustively searches over a specified parameter grid for the best combination of hyperparameters. For each combination:\n",
        "\n",
        "1. It trains the model multiple times (defined by cv, typically 5-fold cross-validation).\n",
        "\n",
        "2. It evaluates the model's performance using a specified scoring metric (in your case, 'recall').\n",
        "\n",
        "3. After trying all combinations, it selects the set of hyperparameters that resulted in the best average score across the cross-validation folds.\n",
        "\n",
        "Why GridSearchCV Was Chosen:\n",
        "\n",
        "1. Exhaustive Search: GridSearchCV guarantees that it will find the best combination of hyperparameters within the defined search space. This is valuable when you have a relatively small and well-defined set of hyperparameters to tune, as it ensures you don't miss optimal configurations.\n",
        "\n",
        "2. Cross-Validation for Robustness: By performing cross-validation (cv=5 for Logistic Regression, cv=3 for XGBoost), it provides a more robust estimate of model performance for each hyperparameter combination. This helps in selecting hyperparameters that generalize well to unseen data, rather than just performing well on a single train/validation split.\n",
        "\n",
        "3. Direct Optimization for Key Metric: You explicitly set scoring='recall'. This is a crucial choice, especially in imbalanced datasets (which you identified earlier). By optimizing for recall, you prioritize minimizing False Negatives (i.e., correctly identifying as many actual positive sentiments as possible). This is often vital in sentiment analysis, where missing negative feedback could have significant business implications.\n",
        "\n",
        "4. Parallel Processing: The use of n_jobs=-1 allows GridSearchCV to run computations in parallel, significantly speeding up the process, especially when the parameter grid is large or cross-validation folds are numerous.\n",
        "\n",
        "While other techniques like Randomized Search (RandomizedSearchCV) might be more efficient for very large search spaces, GridSearchCV is effective for smaller, targeted searches as performed here, especially when you want to ensure the absolute best combination within the defined ranges is found."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there is an improvement in model performance after applying hyperparameter optimization with GridSearchCV.\n",
        "\n",
        "The \"improvement\" needs to be understood in the context of the scoring metric used for GridSearchCV, which was 'recall'.\n",
        "\n",
        "1. Logistic Regression\n",
        "\n",
        "* Targeted Improvement: For Logistic Regression, the Recall score significantly improved from 0.9333 to 0.9667. This is a direct consequence of optimizing for 'recall' during GridSearchCV. A higher recall means the model is better at identifying all actual positive cases, which is crucial if minimizing false negatives is important (e.g., catching all instances of negative feedback).\n",
        "\n",
        "* Trade-offs: This improvement in recall came at the cost of a decrease in other metrics like ROC AUC, Precision, F1-score, and Accuracy. The increase in False Positives (from 183 to 320) while reducing False Negatives (from 82 to 41) indicates this trade-off. The model became more aggressive in predicting the positive class.\n",
        "\n",
        "2. XGBoost\n",
        "\n",
        "  * Overall Improvement: For XGBoost, there's a more balanced improvement. ROC AUC, Precision, F1-score, and Accuracy all increased.\n",
        "\n",
        "  * ROC AUC improved from 0.7603 to 0.8181, indicating better overall discriminative power.\n",
        "\n",
        "  * Precision improved from 0.7838 to 0.8481, meaning fewer false positives.\n",
        "\n",
        "  * F1-score improved from 0.8538 to 0.8706, showing a better balance between precision and recall.\n",
        "\n",
        "  * Accuracy improved from 0.8018 to 0.8359, indicating more correct predictions overall.\n",
        "\n",
        "* Recall Trade-off: Interestingly, even though the primary scoring for optimization was recall, XGBoost's recall slightly decreased (from 0.9374 to 0.8943). This could be due to the interaction of max_depth and n_estimators with the recall scoring, where the best combination for recall in cross-validation might not perfectly align with the test set's recall, or the other metrics saw larger gains. The confusion matrix shows an increase in False Negatives (from 77 to 130), supporting the drop in recall.\n",
        "\n",
        "In conclusion, GridSearchCV successfully optimized for the chosen metric (recall for Logistic Regression, and recall for XGBoost leading to overall better performance for XGBoost). The \"improvement\" is evident in the targeted recall for Logistic Regression and a general uplift across most metrics for XGBoost, demonstrating the value of hyperparameter tuning."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating variable that contain restaurant cuisine details\n",
        "restaurant_df = cluster_dummy.copy()\n",
        "restaurant_df = restaurant_df.reset_index()\n",
        "restaurant_df = restaurant_df.drop(columns = ['Cost',\t'Average_Rating',\t'Total_Cuisine_Count','label'], axis =1)\n",
        "restaurant_df.head(2)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shape\n",
        "restaurant_df.shape"
      ],
      "metadata": {
        "id": "mNrthHP1jwwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#restaurant matrix\n",
        "rest_genre = restaurant_df.loc[:, restaurant_df.columns != 'Restaurant']\n",
        "rest_matrix = rest_genre.values\n",
        "rest_matrix"
      ],
      "metadata": {
        "id": "paAG_fvlj0ER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#matrix shape\n",
        "rest_matrix.shape"
      ],
      "metadata": {
        "id": "TTykModgj3vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Or CountVectorizer\n",
        "\n",
        "# --- Ensure 'review' DataFrame exists (your raw review data) ---\n",
        "try:\n",
        "    _ = review\n",
        "except NameError:\n",
        "    # If 'review' is not defined, create a dummy DataFrame for demonstration.\n",
        "    # In your actual notebook, this would be your loaded review data.\n",
        "    review = pd.DataFrame({\n",
        "        'Restaurant': ['Taste Buds', 'Spice Route', 'Green Oasis'],\n",
        "        'Reviewer': ['Alice', 'Bob', 'Charlie'],\n",
        "        'Review': [\n",
        "            \"Great food and fantastic service!\",\n",
        "            \"Service was slow, but the ambiance was nice.\",\n",
        "            \"Horrible experience, never again.\",\n",
        "        ],\n",
        "        'Rating': [5.0, 3.0, 1.0]\n",
        "    })\n",
        "    print(\"Dummy 'review' DataFrame created for demonstration.\")\n",
        "\n",
        "# --- Ensure 'X' (vectorized text) and 'vectorizer' exist ---\n",
        "try:\n",
        "    _ = X\n",
        "    _ = vectorizer\n",
        "except NameError:\n",
        "    # If 'X' and 'vectorizer' are not defined, create them by vectorizing 'Review' column.\n",
        "    vectorizer = TfidfVectorizer(max_features=1000) # Use your actual vectorizer parameters\n",
        "    X = vectorizer.fit_transform(review['Review'])\n",
        "    print(\"Dummy 'X' (vectorized reviews) and 'vectorizer' created for demonstration.\")\n",
        "\n",
        "# --- Ensure 'lda' (fitted LDA model) exists ---\n",
        "try:\n",
        "    _ = lda\n",
        "except NameError:\n",
        "    # If 'lda' is not defined, fit a dummy LDA model.\n",
        "    lda = LatentDirichletAllocation(n_components=4, random_state=42) # Use your actual n_components\n",
        "    lda.fit(X)\n",
        "    print(\"Dummy 'lda' model fitted for demonstration.\")\n",
        "\n",
        "# --- Create 'review_sentiment_prediction' (if it's not already) ---\n",
        "# This DataFrame typically holds your original review data plus the predicted topic/sentiment.\n",
        "try:\n",
        "    _ = review_sentiment_prediction\n",
        "except NameError:\n",
        "    review_sentiment_prediction = review.copy()\n",
        "    topic_results = lda.transform(X)\n",
        "    review_sentiment_prediction['Prediction'] = topic_results.argmax(axis=1)\n",
        "    print(\"'review_sentiment_prediction' created from previous steps.\")\n",
        "\n",
        "# --- Corrected code starts here ---\n",
        "# Define 'sentiment_df' from 'review_sentiment_prediction'\n",
        "sentiment_df = review_sentiment_prediction.copy()\n",
        "\n",
        "# creating user or reviewer profile\n",
        "user_df = sentiment_df[['Reviewer', 'Restaurant', 'Rating']].copy()\n",
        "user_df.head()"
      ],
      "metadata": {
        "id": "fvZGwnKMj6wC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shape\n",
        "user_df.shape"
      ],
      "metadata": {
        "id": "rXFxPn6PvEo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grouping the data by the 'user' column\n",
        "grouped_data = user_df.groupby('Reviewer')\n",
        "\n",
        "# defining a function to create the new dataframe\n",
        "def create_new_column(data):\n",
        "    return [{'Restaurant': row['Restaurant'], 'Rating': row['Rating']} for _, row in data.iterrows()]\n",
        "    #variable _ is used to store the index value, which is not used in the loop\n",
        "\n",
        "# applying the function to the grouped data and creating a new dataframe\n",
        "user_rating = grouped_data.apply(create_new_column)\n",
        "user_rating = user_rating.reset_index().rename(columns ={0:'Rated_Restaurant'})\n",
        "user_rating.head()"
      ],
      "metadata": {
        "id": "WzF3-rPqvIkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shape\n",
        "user_rating.shape"
      ],
      "metadata": {
        "id": "Aa3cJ17GvNlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#iterating over user rating df such that it end up making an array which had same shape as restaurant df\n",
        "user_rated_restaurant = {}\n",
        "for index, row in user_rating.iterrows():\n",
        "    user_rated_restaurant[row['Reviewer']] = {}\n",
        "    for i in range(len(row['Rated_Restaurant'])):\n",
        "        user_rated_restaurant[row['Reviewer']][row['Rated_Restaurant'][i][\n",
        "            'Restaurant']] = row['Rated_Restaurant'][i]['Rating']\n",
        "\n",
        "# creating an empty user preference vector for each user\n",
        "user_preference_vector = pd.DataFrame(np.zeros((len(user_rating), len(restaurant_df))),\n",
        "                      columns=restaurant_df.Restaurant, index=user_rating['Reviewer'])\n",
        "\n",
        "# Iterate through the user rating dataframe\n",
        "for index, row in user_rating.iterrows():\n",
        "    for i in range(len(row['Rated_Restaurant'])):\n",
        "        restaurant = row['Rated_Restaurant'][i]['Restaurant']\n",
        "        rating = row['Rated_Restaurant'][i]['Rating']\n",
        "        user_preference_vector.loc[row['Reviewer'], restaurant] = rating\n",
        "\n",
        "#reset index\n",
        "user_preference_vector = user_preference_vector.reset_index()"
      ],
      "metadata": {
        "id": "GjnC7RsxvSq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting output\n",
        "user_preference_vector.sample(5)"
      ],
      "metadata": {
        "id": "76q9TCULvY-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'rest_genre', 'user_preference_vector', and 'rest_matrix' are already defined.\n",
        "# If they are not, you would need to define them before this code block.\n",
        "\n",
        "# Initialize an empty list to collect the DataFrames\n",
        "rows_to_concat = []\n",
        "\n",
        "for index, row in user_preference_vector.iterrows():\n",
        "    # Ensure that row[1:] correctly captures the numerical preference vector.\n",
        "    # If 'Reviewer' is the first column, this is correct.\n",
        "    user_preference_vector_array = row[1:].values.reshape(1, -1)\n",
        "    dot_product = np.dot(user_preference_vector_array, rest_matrix)\n",
        "\n",
        "    # Create a DataFrame for the current reviewer's scores and append to the list\n",
        "    # Use row['Reviewer'] directly for the index when creating the DataFrame\n",
        "    reviewer_scores = pd.DataFrame(dot_product, columns=rest_genre.columns, index=[row['Reviewer']])\n",
        "    rows_to_concat.append(reviewer_scores)\n",
        "\n",
        "# Concatenate all collected DataFrames outside the loop\n",
        "result_df = pd.concat(rows_to_concat)\n",
        "\n",
        "# Reset the index and rename the new index column to 'Reviewer'\n",
        "result_df = result_df.reset_index().rename(columns={'index': 'Reviewer'})"
      ],
      "metadata": {
        "id": "d2yLQf55vdc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting output\n",
        "result_df[:5]"
      ],
      "metadata": {
        "id": "x4d168ncv5rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating test user\n",
        "test_user_ids = user_rating.copy()\n",
        "test_user_ids['Rated_Restaurant_Count'] = test_user_ids['Rated_Restaurant'].apply(lambda x: len(x))\n",
        "\n",
        "#taking 1000 user who atleast rating 2 restaurant as they show repeatition\n",
        "test_user_ids = test_user_ids.sort_values('Rated_Restaurant_Count', ascending = False)[:1000]\n",
        "test_user_ids.head()"
      ],
      "metadata": {
        "id": "XJk_hCVmv-dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating list for all reviewer in test ids\n",
        "test_user_ids = test_user_ids['Reviewer'].to_list()\n",
        "print(f\"Total numbers of test users {len(test_user_ids)}\")"
      ],
      "metadata": {
        "id": "-9V2YgRuwBuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test user profile\n",
        "test_user_profile = result_df[result_df['Reviewer']=='Ankita']\n",
        "test_user_profile"
      ],
      "metadata": {
        "id": "ghPLN9GCwFEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's get the test user vector by excluding the `user` column\n",
        "test_user_vector = test_user_profile.iloc[0, 1:].values\n",
        "test_user_vector"
      ],
      "metadata": {
        "id": "wGQdt7-AwI3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let test reviewer or user be 'Ankita'\n",
        "liked_restaurant = user_df[user_df['Reviewer'] == 'Ankita']['Restaurant'].to_list()\n",
        "liked_restaurant = set(liked_restaurant)\n",
        "liked_restaurant"
      ],
      "metadata": {
        "id": "-lPtwIKwwNRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting values for all restaurant\n",
        "all_restaurant = set(restaurant_df['Restaurant'].values)"
      ],
      "metadata": {
        "id": "BcL8Tt-YwRSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting unknown restaurants\n",
        "unknown_restaurant = all_restaurant.difference(liked_restaurant)"
      ],
      "metadata": {
        "id": "711Iw6fGwVal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting unknown restaurant genre\n",
        "unknown_restaurant_genres = restaurant_df[restaurant_df['Restaurant'].isin(unknown_restaurant)]\n",
        "#getting the restaurant matrix by excluding `Restaurant' columns:\n",
        "restaurant_matrix = unknown_restaurant_genres.iloc[:, 1:].values\n",
        "restaurant_matrix"
      ],
      "metadata": {
        "id": "GZCvXef5wYay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#recommendation score\n",
        "score = np.dot(restaurant_matrix[1], test_user_vector)\n",
        "score"
      ],
      "metadata": {
        "id": "k6x3LrnlwcfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only keep the score larger than the recommendation threshold\n",
        "# The threshold can be fine-tuned to adjust the size of generated recommendations\n",
        "score_threshold = 10.0\n",
        "# score_threshold = 20.0\n",
        "res_dict = {}"
      ],
      "metadata": {
        "id": "bPDzoSHhwiBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_recommendation_scores():\n",
        "    users = []\n",
        "    restaurant = []\n",
        "    scores = []\n",
        "    for user_id in test_user_ids:\n",
        "        test_user_profile = result_df[result_df['Reviewer'] == user_id]\n",
        "        # get user vector for the current user id\n",
        "        test_user_vector = test_user_profile.iloc[0, 1:].values\n",
        "\n",
        "\n",
        "        # get the unknown restaurant ids for the current user id\n",
        "        liked_restaurant = user_df[user_df['Reviewer'] == user_id]['Restaurant'].to_list()\n",
        "        all_restaurant = set(restaurant_df['Restaurant'].values)\n",
        "        unknown_restautant = all_restaurant.difference(liked_restaurant)\n",
        "        unknown_restaurant_genres = restaurant_df[restaurant_df['Restaurant'].isin(unknown_restaurant)]\n",
        "        unknown_restaurant_ids = unknown_restaurant_genres.iloc[:, :1].values\n",
        "\n",
        "        # user np.dot() to get the recommendation scores for each restaurant\n",
        "        recommendation_scores = np.dot(unknown_restaurant_genres.iloc[:, 1:].values, test_user_vector)\n",
        "\n",
        "        # Append the results into the users, restaurant, and scores list\n",
        "        for i in range(0, len(unknown_restaurant_ids)):\n",
        "            score = recommendation_scores[i]\n",
        "            # Only keep the restaurant with high recommendation score\n",
        "            if score >= score_threshold:\n",
        "              users.append(user_id)\n",
        "              restaurant.append(unknown_restaurant_ids[i])\n",
        "              scores.append(recommendation_scores[i])\n",
        "\n",
        "    return users, restaurant, scores"
      ],
      "metadata": {
        "id": "tbvvdgmwwmpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Return users, courses, and scores lists for the dataframe\n",
        "users, restaurant, scores = generate_recommendation_scores()\n",
        "res_dict['User'] = users\n",
        "res_dict['Restaurant'] = restaurant\n",
        "res_dict['Score'] = scores\n",
        "res_df = pd.DataFrame(res_dict, columns=['User', 'Restaurant', 'Score'])\n",
        "res_df['Restaurant'] = res_df['Restaurant'].apply(lambda x: str(x[0]))\n",
        "res_df"
      ],
      "metadata": {
        "id": "eOK4mTwNwqxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#most recommended restaurant\n",
        "recom_rest = res_df.groupby('Restaurant')['User'].count().reset_index().sort_values(\n",
        "                            'User', ascending = False)\n",
        "recom_rest[:5]"
      ],
      "metadata": {
        "id": "f7zqcNIcwv27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#least recommended restaurant\n",
        "recom_rest[-5:]"
      ],
      "metadata": {
        "id": "_DtSU396wzRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grouping the data by the 'user' column\n",
        "grouped_data = res_df.groupby('User')\n",
        "\n",
        "# defining a function to create the new dataframe\n",
        "def create_new_column(data):\n",
        "    return [{'Restaurant': row['Restaurant'], 'Score': row['Score']} for _, row in data.iterrows()]\n",
        "    #variable _ is used to store the index value, which is not used in the loop\n",
        "\n",
        "# applying the function to the grouped data and creating a new dataframe\n",
        "recommendation = grouped_data.apply(create_new_column)\n",
        "recommendation = recommendation.reset_index().rename(columns ={0:'Recommended_Restaurant'})\n",
        "recommendation.head()"
      ],
      "metadata": {
        "id": "Rh8oFTx8w3IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating column for total recommendation count for each user\n",
        "recommendation['Total_Recommendation'] = recommendation['Recommended_Restaurant'].apply(\n",
        "    lambda x: len(x))\n",
        "\n",
        "#top 10 user who get most recommendation\n",
        "recommendation.sort_values('Total_Recommendation', ascending= False)[:10]"
      ],
      "metadata": {
        "id": "TjSzwkf2w7k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating new dataframe for recommendation for test user\n",
        "for i in recommendation[recommendation['User']=='Ankita']['Recommended_Restaurant']:\n",
        "    # creating the dataframe\n",
        "    vis = pd.DataFrame(i, columns = ['Restaurant', 'Score'])\n",
        "vis.sort_values('Score', ascending = False)"
      ],
      "metadata": {
        "id": "SrYC3olDw_mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bag of word with doc index as these index will be used for finding similarity later\n",
        "bows_df.sample(5)"
      ],
      "metadata": {
        "id": "dHVIqanAxDp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using extracted bag of words\n",
        "bow_df = bows_df.drop(columns = ['doc_index'], axis =1)\n",
        "bow_df.head()"
      ],
      "metadata": {
        "id": "czqbP4texHta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Restaurant and review\n",
        "rest_review = sentiment_df[['Restaurant','Review']].copy()\n",
        "rest_review.sample(5)"
      ],
      "metadata": {
        "id": "f6ERIbNzxLt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bag of words for restaurant 'Asian Meal Box'\n",
        "rest_bow = bow_df[bow_df['doc_id'] == 'Asian Meal Box']\n",
        "rest_bow[:10]"
      ],
      "metadata": {
        "id": "wKS_K_nqxPB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting bow to horizontal format using pivot\n",
        "rest_bowT = rest_bow.pivot_table(index=['doc_id'], columns=['token'],\n",
        "                                  aggfunc='sum').reset_index(level=[0])\n",
        "rest_bowT"
      ],
      "metadata": {
        "id": "Ps33pg4dxS1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using union set to compare two restaurant set of tokens\n",
        "def pivot_two_bows(basedoc, comparedoc):\n",
        "    base = basedoc.copy()\n",
        "    base['type'] = 'base'\n",
        "    compare = comparedoc.copy()\n",
        "    compare['type'] = 'compare'\n",
        "    # append the two token sets vertically\n",
        "    join = base.append(compare)\n",
        "    # pivot the two joined courses\n",
        "    joinT = join.pivot_table(index=['doc_id', 'type'], columns='token',\n",
        "              aggfunc='sum').fillna(0).reset_index(level=[0, 1])\n",
        "    # assign columns\n",
        "    joinT.columns = ['doc_id', 'type'] + [t[1] for t in joinT.columns][2:]\n",
        "    return joinT"
      ],
      "metadata": {
        "id": "-8l5XiJDxcrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating two test restaurant\n",
        "rest1 = bow_df[bow_df['doc_id'] == 'Asian Meal Box']\n",
        "rest2 = bow_df[bow_df['doc_id'] == 'Biryanis And More']"
      ],
      "metadata": {
        "id": "b-iMrx0oxgpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "def pivot_two_bows(df1, df2):\n",
        "\n",
        "    all_rows_to_concat = []\n",
        "\n",
        "    if not df1.empty: # Example condition\n",
        "        all_rows_to_concat.append(df1)\n",
        "    if not df2.empty: # Example condition\n",
        "        all_rows_to_concat.append(df2)\n",
        "\n",
        "    if all_rows_to_concat:\n",
        "        # Use pd.concat() to combine all accumulated DataFrames at once\n",
        "        final_df = pd.concat(all_rows_to_concat, ignore_index=False) # Adjust ignore_index as needed\n",
        "    else:\n",
        "        final_df = pd.DataFrame() # Return an empty DataFrame if nothing was added\n",
        "\n",
        "    return final_df\n",
        "\n",
        "bow_vectors = pivot_two_bows(rest1, rest2)"
      ],
      "metadata": {
        "id": "uy7EgWikxi80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "#calculating similarity between two restaurant\n",
        "similarity = 1 - cosine(bow_vectors.iloc[0, 2:], bow_vectors.iloc[1, 2:])\n",
        "\n",
        "similarity"
      ],
      "metadata": {
        "id": "TPV2PTCexprv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "#creating function to calculate cosine similarity such that matrix can be made for each restaurant similarity\n",
        "\n",
        "# Get the list of all restaurant\n",
        "all_restaurant = rest_review['Restaurant'].unique()\n",
        "\n",
        "# Initialize the dataframe to store the similarities\n",
        "df_similarities = pd.DataFrame(columns = all_restaurant, index = all_restaurant)\n",
        "\n",
        "# Iterate over the rows and columns of the dataframe\n",
        "for i in all_restaurant:\n",
        "    for j in all_restaurant:\n",
        "        # Get the BoW representation of the current row and column restaurant\n",
        "        #creating two test restaurant\n",
        "        rest1 = bow_df[bow_df['doc_id'] == i]\n",
        "        rest2 = bow_df[bow_df['doc_id'] == j]\n",
        "        bow_vectors = pivot_two_bows(rest1, rest2)\n",
        "        # Calculate the cosine similarity between the two restaurant' BoW representations\n",
        "        sim = 1 - cosine(bow_vectors.iloc[0, 2:], bow_vectors.iloc[1, 2:])\n",
        "        # Assign the similarity score to the corresponding cell of the dataframe\n",
        "        df_similarities.at[i, j] = sim"
      ],
      "metadata": {
        "id": "eZOrcwHCyMF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating function for mapping\n",
        "# Create restaurant id to index and index to id mappings\n",
        "def get_doc_dicts(bow_df):\n",
        "    grouped_df = bow_df.groupby(['doc_id']).max().reset_index(drop=False)\n",
        "    idx_id_dict = grouped_df[['doc_id']].to_dict()['doc_id']\n",
        "    id_idx_dict = {v: k for k, v in idx_id_dict.items()}\n",
        "    del grouped_df\n",
        "    return idx_id_dict, id_idx_dict"
      ],
      "metadata": {
        "id": "Eksp1FdLzzv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#two test subject\n",
        "rest1 = rest_review[rest_review['Restaurant'] == \"Beyond Flavours\"]\n",
        "rest2 = rest_review[rest_review['Restaurant'] == \"Paradise\"]"
      ],
      "metadata": {
        "id": "8JBCULKwz4UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with restaurant name finding index for similarity\n",
        "idx_id_dict, id_idx_dict = get_doc_dicts(bows_df)\n",
        "idx1 = id_idx_dict[\"Beyond Flavours\"]\n",
        "idx2 = id_idx_dict[\"Paradise\"]\n",
        "print(f\"Restaurant 1's index is {idx1} and Restaurant 2's index is {idx2}\")"
      ],
      "metadata": {
        "id": "GXnq_UdUz-q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#locating in the similarti df\n",
        "sim_matrix = df_similarities.to_numpy()\n",
        "\n",
        "#similarity between the two restaurant\n",
        "sim = sim_matrix[idx1][idx2]\n",
        "sim"
      ],
      "metadata": {
        "id": "U97d1tRH0DoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to recommend restaurant based on similarity\n",
        "def generate_recommendations_for_one_user(liked_restaurant, unknown_restaurant, id_idx_dict, sim_matrix):\n",
        "    # Create a dictionary to store your recommendation results\n",
        "    res = {}\n",
        "    threshold = 0.6\n",
        "    for liked_rest in liked_restaurant:\n",
        "        for unselect_rest in unknown_restaurant:\n",
        "            if liked_rest in id_idx_dict and unselect_rest in id_idx_dict:\n",
        "                sim = 0\n",
        "                idx1 = id_idx_dict[liked_rest]\n",
        "                idx2 = id_idx_dict[unselect_rest]\n",
        "\n",
        "                # Find the similarity value from the sim_matrix\n",
        "                sim = sim_matrix[idx1][idx2]\n",
        "                if sim > threshold:\n",
        "                    if unselect_rest not in res:\n",
        "                        res[unselect_rest] = sim\n",
        "                    else:\n",
        "                        if sim >= res[unselect_rest]:\n",
        "                            res[unselect_rest] = sim\n",
        "\n",
        "    # Sort the results by similarity\n",
        "    res = {k: v for k, v in sorted(res.items(), key=lambda item: item[1], reverse=True)}\n",
        "    return res"
      ],
      "metadata": {
        "id": "WvWH5uFk0JNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to calculate recommendation for all Reviewer\n",
        "def generate_recommendations_for_all():\n",
        "    users = []\n",
        "    restaurant = []\n",
        "    sim_scores = []\n",
        "    idx_id_dict, id_idx_dict = get_doc_dicts(bows_df)\n",
        "    sim_matrix = df_similarities.to_numpy()\n",
        "    all_restaurant = set(restaurant_df['Restaurant'])\n",
        "    for user_id in test_user_ids:\n",
        "        liked_restaurant = user_df[user_df['Reviewer'] == user_id]['Restaurant'].to_list()\n",
        "        unknown_restaurant = all_restaurant.difference(liked_restaurant)\n",
        "        rec = generate_recommendations_for_one_user(liked_restaurant, unknown_restaurant, id_idx_dict, sim_matrix)\n",
        "        for k, v in rec.items():\n",
        "            users.append(user_id)\n",
        "            restaurant.append(k)\n",
        "            sim_scores.append(v)\n",
        "\n",
        "    return users, restaurant, sim_scores"
      ],
      "metadata": {
        "id": "Ak_A5aQt0OjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#storing recommendation for each user in dataframe\n",
        "res_sim_dict = {}\n",
        "users, restaurant, sim_scores = generate_recommendations_for_all()\n",
        "res_sim_dict['USER'] = users\n",
        "res_sim_dict['RESTAURANT'] = restaurant\n",
        "res_sim_dict['SCORE'] = sim_scores\n",
        "res_sim_df = pd.DataFrame(res_sim_dict, columns=['USER', 'RESTAURANT', 'SCORE'])"
      ],
      "metadata": {
        "id": "iTG3jHPM0T_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting the output\n",
        "res_sim_df.sample(10)"
      ],
      "metadata": {
        "id": "LtjibeB60XIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ML Models Used:\n",
        "\n",
        "I used two supervised machine learning models for sentiment analysis:\n",
        "\n",
        "1. Logistic Regression\n",
        "\n",
        "This is a linear model that predicts the probability of a binary outcome (e.g., positive or negative sentiment). It uses a sigmoid function to map the input features to a probability score. It's a fundamental classification algorithm, simple yet effective, especially for linearly separable data.\n",
        "\n",
        "2. XGBoost (Extreme Gradient Boosting)\n",
        "\n",
        "This is an advanced ensemble learning method based on gradient boosting. It builds multiple decision trees sequentially, with each new tree trying to correct the errors of the previous ones. XGBoost is highly efficient, flexible, and known for its strong performance in various machine learning tasks due to its robust handling of complex relationships and built-in regularization to prevent overfitting.\n",
        "\n",
        "Performance Evaluation:\n",
        "\n",
        "Model performance was evaluated using several key metrics, which provide a comprehensive view of how well each model classifies sentiments. The chosen metrics were particularly relevant given the nature of classification and the possibility of class imbalance in sentiment data:\n",
        "\n",
        "1. ROC AUC (Receiver Operating Characteristic Area Under the Curve)\n",
        "\n",
        "This metric assesses the model's ability to distinguish between positive and negative classes. A higher ROC AUC indicates better overall discriminative power.\n",
        "\n",
        "2. Precision\n",
        "\n",
        "This measures the accuracy of positive predictions. It tells us, \"Of all the instances predicted as positive, how many were actually positive?\" High precision means fewer false positives.\n",
        "\n",
        "3. Recall (Sensitivity)\n",
        "\n",
        "This measures the model's ability to find all actual positive instances. It tells us, \"Of all the actual positive instances, how many did the model correctly identify?\" High recall means fewer false negatives.\n",
        "\n",
        "4. F1-Score\n",
        "\n",
        "This is the harmonic mean of precision and recall. It provides a single score that balances both precision and recall, being particularly useful when there's an uneven class distribution.\n",
        "\n",
        "5. Accuracy\n",
        "\n",
        "This is the ratio of correctly predicted instances to the total number of instances. While a general indicator of performance, it can be misleading if the dataset is imbalanced.\n",
        "\n",
        "6. Confusion Matrix\n",
        "\n",
        "This table provides a detailed breakdown of correct and incorrect predictions, showing True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN).\n",
        "\n",
        "Hyperparameter Optimization Technique:\n",
        "\n",
        "I used Grid Search Cross-Validation (GridSearchCV) for hyperparameter optimization for both Logistic Regression and XGBoost.\n",
        "\n",
        "Why GridSearchCV?\n",
        "\n",
        "1. Exhaustive Search\n",
        "\n",
        "GridSearchCV systematically explores all possible combinations of hyperparameters defined within a specified grid. This ensures that the optimal set of parameters within the chosen range is found.\n",
        "\n",
        "2. Robust Evaluation with Cross-Validation\n",
        "\n",
        "By integrating cross-validation, GridSearchCV trains and evaluates the model multiple times on different subsets of the data. This helps in selecting hyperparameters that lead to better generalization performance on unseen data, reducing the risk of overfitting to a single training set.\n",
        "\n",
        "3. Targeted Metric Optimization\n",
        "\n",
        "For both models, the scoring parameter was set to 'recall'. This means GridSearchCV specifically aimed to find hyperparameter combinations that maximize the model's recall score. This is crucial in sentiment analysis, especially when identifying all instances of a particular sentiment (e.g., negative reviews to address customer complaints) is a high priority.\n",
        "\n",
        "4. Parallelization\n",
        "\n",
        "The n_jobs=-1 parameter allowed the computation to be distributed across all available CPU cores, significantly speeding up the hyperparameter tuning process.\n",
        "\n",
        "While other methods like Randomized Search could be faster for very large search spaces, Grid Search was chosen for its thoroughness given the moderately sized parameter grids.\n",
        "\n",
        "Improvement After Hyperparameter Tuning:\n",
        "\n",
        "Hyperparameter tuning with GridSearchCV led to noticeable improvements in model performance, particularly aligned with the chosen optimization metric:\n",
        "\n",
        "* For Logistic Regression: The most significant improvement was observed in Recall. By optimizing for this metric, the model became more effective at identifying true positive sentiments, leading to a higher recall score. This enhancement was a direct result of fine-tuning the C (regularization strength) and penalty parameters. While recall improved, there were some trade-offs in other metrics like precision and accuracy, indicating a more aggressive classification towards the positive class to minimize false negatives.\n",
        "\n",
        "* For XGBoost: Tuning brought about a more balanced improvement across several metrics. The ROC AUC, Precision, F1-Score, and Accuracy all showed increases. This suggests that optimizing parameters like n_estimators, max_depth, and criterion resulted in a more robust and generally better-performing model. Although recall saw a slight decrease in the tuned XGBoost compared to its untuned version, the overall gains in other key metrics indicate a stronger classifier with better generalization capabilities.\n",
        "\n",
        "In essence, hyperparameter tuning successfully enhanced the models' ability to classify sentiments, either by directly improving the targeted recall or by boosting overall performance across multiple evaluation metrics."
      ],
      "metadata": {
        "id": "oPWE0pOiZMH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import numpy as np # Ensure numpy is imported if needed for X_test later\n",
        "\n",
        "# Get shap values\n",
        "best_xgb_model = xgb_grid.best_estimator_\n",
        "\n",
        "# explainer = shap.Explainer(best_xgb_model.predict_proba, X_test) # If you want to explain probabilities\n",
        "explainer = shap.TreeExplainer(best_xgb_model) # More efficient and specific for tree models like XGBoost\n",
        "\n",
        "shap_values = explainer(X_test)"
      ],
      "metadata": {
        "id": "dfZNyK_YDSFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Waterfall plot for first observation\n",
        "shap.plots.waterfall(shap_values[0])"
      ],
      "metadata": {
        "id": "W3AV3mcOERro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize JavaScript visualizations in notebook environment\n",
        "shap.initjs()\n",
        "# Forceplot for first observation\n",
        "shap.plots.force(shap_values[0])"
      ],
      "metadata": {
        "id": "_1U76ovmEeP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mean SHAP\n",
        "shap.plots.bar(shap_values)"
      ],
      "metadata": {
        "id": "a9JnFeEdE4gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Beeswarm plot\n",
        "shap.plots.beeswarm(shap_values)"
      ],
      "metadata": {
        "id": "z9F8MGLlFAwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ML Model Used: XGBoost Classifier\n",
        "\n",
        "I've used the XGBoost Classifier as the final prediction model for sentiment analysis.\n",
        "\n",
        "What is XGBoost?\n",
        "\n",
        "XGBoost (Extreme Gradient Boosting) is an advanced and highly efficient implementation of Gradient Boosting Machines (GBMs). It's an ensemble learning method that constructs a strong predictive model by combining the predictions of numerous weak, sequential decision tree models.\n",
        "\n",
        "Key characteristics and why it was chosen:\n",
        "\n",
        "1. Sequential Learning: XGBoost builds trees one after another, where each new tree aims to correct the errors (residuals) made by the ensemble of previous trees. This iterative error correction leads to highly accurate models.\n",
        "\n",
        "2. Gradient Descent Optimization: It utilizes a gradient descent framework to systematically minimize the loss function during the tree-building process.\n",
        "\n",
        "3. Regularization: A critical strength of XGBoost is its built-in L1 (Lasso) and L2 (Ridge) regularization techniques. These mechanisms penalize complex models, effectively preventing overfitting and improving the model's ability to generalize to unseen data.\n",
        "\n",
        "4. Missing Value Handling: XGBoost can inherently handle missing values in the dataset by learning the best direction to take when a value is absent.\n",
        "\n",
        "5. Performance and Speed: It's renowned for its computational efficiency and often achieves state-of-the-art performance in various machine learning competitions and real-world applications.\n",
        "\n",
        "I chose XGBoost due to its robustness, speed, and consistently high performance, which was evident in its superior and more balanced evaluation metrics (e.g., higher ROC AUC, Precision, F1-Score, and Accuracy) after hyperparameter tuning, compared to Logistic Regression.\n",
        "\n",
        "Feature Importance using SHAP (SHapley Additive exPlanations)\n",
        "\n",
        "To gain deep insights into why the XGBoost model makes specific sentiment predictions and to understand the overall importance of different features, I used SHAP values. SHAP is a powerful and unified framework for explaining the output of any machine learning model.\n",
        "\n",
        "How SHAP Works:\n",
        "\n",
        "SHAP values are rooted in Shapley values from cooperative game theory. For each prediction made by the model, SHAP calculates how much each feature contributes to pushing the prediction from the average (base) prediction to the actual prediction for that specific instance.\n",
        "\n",
        "1. Fair Attribution: SHAP fairly distributes the credit (or blame) for a prediction among all features, considering all possible permutations in which features could have been introduced to the model.\n",
        "\n",
        "2. Additive Explanations: The SHAP values for all features for a single prediction sum up to the difference between the model's output for that prediction and the base value.\n",
        "\n",
        "3. Local and Global Interpretability:\n",
        "\n",
        "* Local Explanations (e.g., Waterfall Plot, Force Plot): For an individual prediction, SHAP can show exactly which features are pushing the prediction higher (positive SHAP value) or lower (negative SHAP value), and by what magnitude.\n",
        "\n",
        "* Global Explanations (e.g., Bar Plot, Beeswarm Plot): By aggregating SHAP values across many predictions, we can understand the overall importance of each feature for the entire model. Features with larger average absolute SHAP values are considered more influential.\n",
        "\n",
        "SHAP Plots and Their Interpretation:\n",
        "\n",
        "1. Waterfall Plot (shap.plots.waterfall(shap_values[0]))\n",
        "\n",
        "* Purpose: This plot explains a single prediction (e.g., for the first test observation).\n",
        "\n",
        "* Interpretation from your description:\n",
        "\n",
        "  1. Base Value (E[f(x)] = 0.584): This is the average (expected) output of the model across the entire dataset. It's the starting point for the explanation.\n",
        "\n",
        "  2. Ending Value (f(x) = -0.382): This is the actual prediction for the specific instance being explained.\n",
        "\n",
        "  3. Feature Contributions: Each bar represents a feature. Bars extending to the right (positive SHAP values) indicate features that push the prediction higher than the base value. Bars extending to the left (negative SHAP values) indicate features that push the prediction lower. The length of the bar shows the magnitude of the impact.\n",
        "\n",
        "  4. Unique per Observation: As you correctly state, there's a unique waterfall plot for every observation, showing how features influenced that specific prediction compared to the mean prediction. Large SHAP values mean significant impact.\n",
        "\n",
        "2. Force Plot (shap.plots.force(shap_values[0]))\n",
        "\n",
        "* Purpose: Also explains a single prediction, offering an interactive visualization (though currently omitted due to environment limitations).\n",
        "\n",
        "* Interpretation from your description:\n",
        "\n",
        "  1. Similar Information: It conveys similar information to the waterfall plot but in a linear, interactive format.\n",
        "\n",
        "  2. Magnitude and Direction: Features pushing the prediction higher than the base value appear on one side (e.g., red, pushing right), and features pushing it lower appear on the other (e.g., blue, pushing left). The size of the feature's contribution indicates its impact.\n",
        "\n",
        "  3. Interactive Relationship: As you noted, it visually represents how features \"compress\" or \"expand\" to reach the final prediction from the base value.\n",
        "\n",
        "3. Bar Plot (shap.plots.bar(shap_values))\n",
        "\n",
        "* Purpose: Provides a global explanation by showing the overall average impact of each feature across the entire dataset.\n",
        "\n",
        "* Interpretation: This plot typically ranks features by the average absolute SHAP value. The longer the bar, the more important the feature is to the model's predictions overall. It tells you which features are generally most influential, regardless of whether they push predictions higher or lower.\n",
        "\n",
        "4. Beeswarm Plot (shap.plots.beeswarm(shap_values))\n",
        "\n",
        "* Purpose: A more detailed global explanation that shows the distribution of SHAP values for each feature.\n",
        "\n",
        "* Interpretation:\n",
        "\n",
        "  1. Feature Importance: Features are typically ordered by their overall importance (like in the bar plot).\n",
        "\n",
        "  2. Impact Direction and Distribution: Each dot represents a single data point's SHAP value for a particular feature. The color usually indicates the feature's actual value (e.g., red for high, blue for low).\n",
        "\n",
        "  3. Insights: This plot helps identify:\n",
        "\n",
        "  * Whether a high value of a feature generally leads to a high/low prediction, and vice-versa.\n",
        "\n",
        "  * If the impact of a feature is consistent or varies across different instances.\n",
        "\n",
        "  * Potential interaction effects with other features (though more directly seen in dependence plots).\n",
        "\n",
        "Why SHAP for XGBoost?\n",
        "\n",
        "1. Tree-Specific Optimization: SHAP has highly optimized and accurate algorithms for tree-based models like XGBoost, making the computation efficient.\n",
        "\n",
        "2. Consistency and Accuracy: SHAP values satisfy desirable properties that make them reliable for interpreting complex black-box models.\n",
        "\n",
        "3. Comprehensive Insight: It offers both instance-level (local) and overall model (global) explanations, providing a holistic understanding of feature contributions. For sentiment analysis, this means not only knowing which words are important but also how certain words (or their frequency/TF-IDF scores) specifically push a review towards a positive or negative prediction for an individual review, and generally across all reviews."
      ],
      "metadata": {
        "id": "sPYptpz8FwYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}